# BEV检测
* [Fast-BEV: A Fast and Strong Bird's-Eye View Perception Baseline](https://arxiv.org/abs/2301.12511) [[Code]](https://github.com/Sense-GVT/Fast-BEV) [[资料]](https://mp.weixin.qq.com/s/Dv8UVrRN2TrOTQCqVtDSJg)
* [M2BEV: Multi-Camera Joint 3D Detection and Segmentation with Unified Bird’s-Eye View Representation](https://arxiv.org/pdf/2204.05088.pdf) [[阅读笔记]](https://zhuanlan.zhihu.com/p/651960376)
* [MatrixVT: Efficient Multi-Camera to BEV Transformation for 3D Perception](https://arxiv.org/abs/2211.10593)
* [BEV-IO: Enhancing Bird’s-Eye-View 3D Detection with Instance Occupancy](https://arxiv.org/pdf/2305.16829.pdf)
* [PETRv2: A Unified Framework for 3D Perception from Multi-Camera Images](https://arxiv.org/pdf/2206.01256.pdf)
* [StreamPetr: Exploring Object-Centric Temporal Modeling for Efficient Multi-View 3D Object Detection](https://arxiv.org/pdf/2303.11926.pdf)
* [OCBEV Object-Centric BEV Transformer for Multi-View 3D Object Detection](https://arxiv.org/pdf/2306.01738.pdf)
* [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation](https://arxiv.org/pdf/2308.09244.pdf)
* [MonoNeRD: NeRF-like Representations for Monocular 3D Object Detection](https://arxiv.org/pdf/2308.09421.pdf)
* [Temporal Enhanced Training of Multi-view 3D Object Detector via Historical Object Prediction](https://arxiv.org/pdf/2304.00967.pdf)
* [DFA3D: 3D Deformable Attention For 2D-to-3D Feature Lifting](https://arxiv.org/pdf/2307.12972.pdf)
* [EA-LSS: Edge-aware Lift-splat-shot Framework for 3D BEV Object Detection](https://arxiv.org/abs/2303.17895)
* [Cross Modal Transformer: Towards Fast and Robust 3D Object Detection](https://arxiv.org/pdf/2301.01283.pdf)
* [BEVHeight++: Toward Robust Visual Centric 3D Object Detection](https://arxiv.org/pdf/2309.16179.pdf)
* [DistillBEV: Boosting Multi-Camera 3D Object Detection with Cross-Modal Knowledge Distillation](https://browse.arxiv.org/pdf/2309.15109.pdf)
* [DynamicBEV: Leveraging Dynamic Queries and Temporal Context for 3D Object Detection.](https://openreview.net/pdf?id=rljudc4XHW)
* [Leveraging Vision-Centric Multi-Modal Expertise for 3D Object Detection](https://arxiv.org/pdf/2310.15670.pdf)
* [BEVStereo: Enhancing Depth Estimation in Multi-view 3D Object Detection with Dynamic Temporal Stereo](https://arxiv.org/pdf/2209.10248.pdf)
* [SOLOFusion Time Will Tell New Outlooks and A Baseline for Temporal Multi-View 3D Object Detection](https://arxiv.org/pdf/2210.02443.pdf)
* [VideoBEV_Exploring Recurrent Long-term Temporal Fusion for Multi-view 3D Perception](https://arxiv.org/pdf/2303.05970.pdf)
* [Long Range: Far3D: Expanding the Horizon for Surround-view 3D Object Detection](https://arxiv.org/pdf/2308.09616.pdf)
* [Sparse4D: Multi-view 3D Object Detection with Sparse Spatial-Temporal Fusion](https://arxiv.org/pdf/2211.10581.pdf)
* [Sparse4D v2: Recurrent Temporal Fusion with Sparse Model](https://arxiv.org/pdf/2305.14018.pdf)
* [Sparse4D v3: Advancing End-to-End 3D Detection and Tracking](https://arxiv.org/pdf/2311.11722.pdf)
* [Towards Efficient 3D Object Detection in Bird’s-Eye-View Space for Autonomous Driving: A Convolutional-Only Approach](https://arxiv.org/pdf/2312.00633.pdf)
* [PointBeV: A Sparse Approach to BeV Predictions](https://arxiv.org/pdf/2312.00703.pdf)
* [Simple-BEV: What Really Matters for Multi-Sensor BEV Perception?](https://arxiv.org/pdf/2206.07959.pdf)
* [WidthFormer: Toward Efficient Transformer-based BEV View Transformation](https://arxiv.org/pdf/2401.03836.pdf)
* [SparseBEV: High-Performance Sparse 3D Object Detection from Multi-Camera Videos](https://arxiv.org/pdf/2308.09244.pdf)
* [UniVision: A Unified Framework for Vision-Centric 3D Perception](https://arxiv.org/pdf/2401.06994.pdf)
* [SAFDNet: A Simple and Effective Network for Fully Sparse 3D Object Detection](https://arxiv.org/pdf/2403.05817.pdf)
* [Lifting Multi-View Detection and Tracking to the Bird’s Eye View](https://arxiv.org/pdf/2403.12573.pdf)
* [M2DA: Multi-Modal Fusion Transformer Incorporating Driver Attention for Autonomous Driving](https://arxiv.org/pdf/2403.12552.pdf)
* [SparseFusion: Efficient Sparse Multi-Modal Fusion Framework for Long-Range 3D Perception](https://arxiv.org/pdf/2403.10036.pdf)
* [BEVCar: Camera-Radar Fusion for BEV Map and Object Segmentation](https://arxiv.org/pdf/2403.11761.pdf)
* [GaussianBeV : 3D Gaussian Representation meets Perception Models for BeV Segmentation](https://arxiv.org/pdf/2407.14108)
* [FlatFusion: Delving into Details of Sparse Transformer-based Camera-LiDAR Fusion for Autonomous Driving](https://arxiv.org/pdf/2408.06832)
* [Vision-Driven 2D Supervised Fine-Tuning Framework for Bird’s Eye View Perception](https://arxiv.org/pdf/2409.05834)
* [UniDrive: Towards Universal Driving Perception Across Camera Configurations](https://arxiv.org/pdf/2410.13864)



# BEV分割 (LaneGraph)
* [Learning and Aggregating Lane Graphs for Urban Automated Driving](https://arxiv.org/abs/2302.06175) 
* [InstaGraM Instance-level Graph Modeling for Vectorized HD Map Learning](https://arxiv.org/abs/2301.04470)
* [(2021) Structured Bird’s-Eye-View Traffic Scene Understanding from Onboard Images](https://arxiv.org/pdf/2110.01997.pdf)
* [(2022) Topology Preserving Local Road Network Estimation from Single Onboard Camera Image](https://openaccess.thecvf.com/content/CVPR2022/papers/Can_Topology_Preserving_Local_Road_Network_Estimation_From_Single_Onboard_Camera_CVPR_2022_paper.pdf)
* [(2022) Lane-Level Street Map Extraction from Aerial Imagery](https://openaccess.thecvf.com/content/WACV2022/papers/He_Lane-Level_Street_Map_Extraction_From_Aerial_Imagery_WACV_2022_paper.pdf)
* [(2023) Lane Graph as Path: Continuity-preserving Path-wise Modeling for Online Lane Graph Construction](https://github.com/hustvl/LaneGAP)
* [MapTR: Structured Modeling and Learning for Online Vectorized HD Map Construction](https://arxiv.org/pdf/2208.14437.pdf) [[Code]](https://github.com/hustvl/MapTR) [[笔记]](https://)
* [Neural Map Prior for Autonomous Driving](https://arxiv.org/pdf/2304.08481.pdf)
* [VMA: Divide-and-Conquer Vectorized Map Annotation System for Large-Scale Driving Scene](https://arxiv.org/pdf/2304.09807.pdf)
* [End-to-End Vectorized HD-map Construction with Piecewise Bezier Curve](https://openaccess.thecvf.com/content/CVPR2023/papers/Qiao_End-to-End_Vectorized_HD-Map_Construction_With_Piecewise_Bezier_Curve_CVPR_2023_paper.pdf)
* [MachMap: End-to-End Vectorized Solution for Compact HD-Map Construction](https://arxiv.org/pdf/2306.10301.pdf)
* [MV-Map: Offboard HD-Map Generation with Multi-view Consistency](https://arxiv.org/pdf/2305.08851.pdf)
* [LATR: 3D Lane Detection from Monocular Images with Transformer](https://arxiv.org/pdf/2308.04583.pdf)
* [MapTRv2: An End-to-End Framework for Online Vectorized HD Map Construction](https://arxiv.org/pdf/2308.05736v1.pdf)
* [InsightMapper: A Closer Look at Inner-instance Information for Vectorized High-Definition Mapping](https://arxiv.org/pdf/2308.08543.pdf)[[Code]]() [[笔记]](https://zhuanlan.zhihu.com/p/650732045)
* [StreamMapNet: Streaming Mapping Network for Vectorized Online HD Map Construction](https://arxiv.org/pdf/2308.12570.pdf)
* [PivotNet: Vectorized Pivot Learning for End-to-end HD Map Construction](https://arxiv.org/pdf/2308.16477.pdf)
* [MLP An Simple yet Strong Pipeline for Driving Topology Reasoning](https://arxiv.org/pdf/2310.06753.pdf)
* [ScalableMap: Scalable Map Learning for Online Long-Range Vectorized HD Map Construction](https://arxiv.org/pdf/2310.13378.pdf)
* [Augmenting Lane Perception and Topology Understanding with Standard Definition Navigation Maps](https://arxiv.org/pdf/2311.04079.pdf)
* [Graph-based Topology Reasoning for Driving Scenes](https://arxiv.org/pdf/2304.05277.pdf)
* [Separated RoadTopoFormer](https://arxiv.org/pdf/2307.01557.pdf)
* [Mind the map! Accounting for existing map information when estimating online HDMaps from sensor data](https://arxiv.org/pdf/2311.10517.pdf)
* [☆X] [Online Vectorized HD Map Construction using Geometry](https://arxiv.org/pdf/2312.03341.pdf)
* [MapNeXt: Revisiting Training and Scaling Practices for Online Vectorized HD Map Construction](https://arxiv.org/pdf/2401.07323.pdf)
* [ADMap: Anti-disturbance framework for reconstructing online vectorized HD](https://arxiv.org/pdf/2401.13172.pdf)
* [High-Definition Maps: Comprehensive Survey, Challenges, and Future Perspectives](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10184094)
* [P-MapNet: Far-seeing Map Constructer Enhanced by both SDMap and HDMap Priors](https://openreview.net/pdf?id=lgDrVM9Rpx)
* [Leveraging Enhanced Queries of Point Sets for Vectorized Map Construction](https://arxiv.org/pdf/2402.17430.pdf)
* [HIMap: HybrId Representation Learning for End-to-end Vectorized HD Map Construction](https://arxiv.org/pdf/2403.08639.pdf)
* [Local map Construction Methods with SD map A Novel Survey](https://arxiv.org/abs/2409.02415)
* [Driving with Prior Maps: Unified Vector Prior Encoding for Autonomous Vehicle Mapping](https://arxiv.org/pdf/2409.05352)
* [Driving by the Rules: A Benchmark for Integrating Traffic Sign Regulations into Vectorized HD Map](https://arxiv.org/pdf/2410.23780v1)
* [Enhancing Lane Segment Perception and Topology Reasoning with Crowdsourcing Trajectory Priors](https://arxiv.org/pdf/2411.17161v1)
* [T2SG: Traffic Topology Scene Graph for Topology Reasoning in Autonomous Driving](https://arxiv.org/pdf/2411.18894v1)
* [FastMap: Fast Queries Initialization Based Vectorized HD Map Reconstruction Framework](https://arxiv.org/pdf/2503.05492)

# BEV Occupancy
* [Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction](https://arxiv.org/pdf/2302.07817.pdf) [[Code]](https://github.com/wzzheng/TPVFormer) [[笔记]](https://zhuanlan.zhihu.com/p/648813002)
* [MonoScene Monocular 3D Semantic Scene Completion](https://arxiv.org/abs/2112.00726)
* [OccDepth: A Depth-aware Method for 3D Semantic Occupancy Network](https://arxiv.org/abs/2302.13540)
* [VoxFormer: Sparse Voxel Transformer for Camera-based 3D Semantic Scene Completion](https://arxiv.org/abs/2302.12251)
* [OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception](https://github.com/JeffWang987/OpenOccupancy)
* [A Simple Attempt for 3D Occupancy Estimation in Autonomous Driving](https://github.com/GANWANSHUI/SimpleOccupancy)
* [Occ-BEV: Multi-Camera Unified Pre-training via 3D Scene Reconstruction](https://arxiv.org/pdf/2305.18829.pdf)
* [Learning Occupancy for Monocular 3D Object Detection](https://arxiv.org/pdf/2305.15694.pdf)
* [Scene as Occupancy](https://arxiv.org/pdf/2306.02851.pdf) [[Code]](https://github.com/OpenDriveLab/OccNet)
* [UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering](https://arxiv.org/pdf/2306.09117.pdf)
* [FB-OCC 3D Occupancy Prediction based on Forward-Backward View Transformation](https://opendrivelab.com/e2ead/AD23Challenge/Track_3_NVOCC.pdf?=&linkId=100000205404832)[[Code]](https://github.com/NVlabs/FB-BEV) [【阅读笔记】](https://zhuanlan.zhihu.com/p/642730470)
* [OVO: Open-Vocabulary Occupancy](https://arxiv.org/pdf/2305.16133.pdf)
* [OCTraN: 3D Occupancy Convolutional Transformer Network in Unstructured Traffic Scenarios](https://arxiv.org/pdf/2307.10934.pdf)
* [RenderOcc: Vision-Centric 3D Occupancy Prediction with 2D Rendering Supervision](https://arxiv.org/pdf/2309.09502.pdf)
* [OccupancyDETR: Making Semantic Scene Completion as Straightforward as Object Detection](https://arxiv.org/pdf/2309.08504.pdf)
* [LiDAR-based 4D Occupancy Completion and Forecasting](https://arxiv.org/pdf/2310.11239.pdf)
* [SelfOcc: Self-Supervised Vision-Based 3D Occupancy Prediction](https://arxiv.org/pdf/2311.12754.pdf)
* [OccNeRF: Self-Supervised Multi-Camera Occupancy Prediction with Neural Radiance Fields](https://arxiv.org/pdf/2312.09243.pdf)
* [Fully Sparse 3D Panoptic Occupancy Prediction](https://arxiv.org/pdf/2312.17118.pdf)
* [FastOcc: Accelerating 3D Occupancy Prediction by Fusing the 2D Bird's-Eye View and Perspective View](https://arxiv.org/pdf/2403.02710.pdf)
* [Real-time 3D semantic occupancy prediction for autonomous vehicles using memory-efficient sparse convolution](https://arxiv.org/pdf/2403.08748.pdf)
* [Co-Occ: Coupling Explicit Feature Fusion with Volume Rendering Regularization for Multi-Modal 3D Semantic Occupancy Prediction](https://arxiv.org/pdf/2404.04561.pdf)
* [SparseOcc: Rethinking Sparse Latent Representation for Vision-Based Semantic Occupancy Prediction](https://arxiv.org/pdf/2404.09502.pdf)
* [OPUS: Occupancy Prediction Using a Sparse Set](https://arxiv.org/pdf/2409.09350)
* [DOME: Taming Diffusion Model into High-Fidelity Controllable Occupancy World Model](https://arxiv.org/pdf/2410.10429)
* [EmbodiedOcc: Embodied 3D Occupancy Prediction for Vision-based Online Scene Understanding](https://arxiv.org/pdf/2412.04380v1)
* [GaussianWorld: Gaussian World Model for Streaming 3D Occupancy Prediction](https://arxiv.org/pdf/2412.10373v1)

# 矢量化
* [PolyFormer Referring Image Segmentation as Sequential Polygon Generation](https://arxiv.org/abs/2302.07387)[[阅读笔记]](https://zhuanlan.zhihu.com/p/650991896)

# 4D label & AutoLableing
* [Label-Efficient 3D Object Detection For Road-Side Units](https://arxiv.org/pdf/2404.06256.pdf)


# 多传感器融合
* [Transformers-Sensor-Fusion](https://github.com/ApoorvRoboticist/Transformers-Sensor-Fusion)
* [UniTR: A Unified and Efficient Multi-Modal Transformer for Bird’s-Eye-View Representation]()
* [DPFT: Dual Perspective Fusion Transformer for Camera-Radar-based Object Detection](https://arxiv.org/pdf/2404.03015.pdf)

# 检测跟踪一体化
* [Exploring Simple 3D Multi-Object Tracking for Autonomous Driving](https://github.com/qcraftai/simtrack)
* [MOTR: End-to-End Multiple-Object Tracking with TRansformer](https://github.com/megvii-research/MOTR)
* [MUTR3D: A Multi-camera Tracking Framework via 3D-to-2D Queries](https://tsinghua-mars-lab.github.io/mutr3d/)
* [MotionTrack: End-to-End Transformer-based Multi-Object Tracing with LiDAR-Camera Fusion](https://arxiv.org/pdf/2306.17000.pdf)
* [3DMOTFormer: Graph Transformer for Online 3D Multi-Object Tracking](https://arxiv.org/pdf/2308.06635.pdf)


# Prediction
* [Motion Transformer with Global Intention Localization and Local Movement Refinement](https://arxiv.org/pdf/2209.13508.pdf)
* [Towards learning-based planning: The nuPlan benchmark for real-world autonomous driving](https://arxiv.org/pdf/2403.04133.pdf)
* [LARGE TRAJECTORY MODELS ARE SCALABLE MOTION PREDICTORS AND PLANNERS](https://arxiv.org/pdf/2310.19620.pdf)
* [Rethinking Imitation-based Planner for Autonomous Driving](https://arxiv.org/pdf/2309.10443)
* [A Multi-Loss Strategy for Vehicle Trajectory Prediction: Combining Off-Road, Diversity, and Directional Consistency Losses](https://arxiv.org/pdf/2411.19747v1)
* [Utilizing Navigation Paths to Generate Target Points for Enhanced End-to-End Autonomous Driving Planning](https://arxiv.org/pdf/2406.08349)


# Localization
* [U-BEV: Height-aware Bird's-Eye-View Segmentation and Neural Map-based Relocalization](https://arxiv.org/pdf/2310.13766.pdf)


# Calibration
* [☆] [Calib-Anything: Zero-training LiDAR-Camera Extrinsic Calibration Method Using Segment Anything](https://arxiv.org/pdf/2306.02656.pdf)

# Simulation 
* [UniSim: A Neural Closed-Loop Sensor Simulator](https://waabi.ai/wp-content/uploads/2023/05/UniSim-paper.pdf)
* [Rethinking Closed-loop Training for Autonomous Driving](https://arxiv.org/pdf/2306.15713.pdf)


# Transformer
* [SegViTv2: Exploring Efficient and Continual Semantic Segmentation with Plain Vision Transformers](https://arxiv.org/pdf/2306.06289.pdf)
* [Taming Transformers for High-Resolution Image Synthesis](https://arxiv.org/pdf/2012.09841.pdf)
* [Mask2Former: Masked-attention Mask Transformer for Universal Image Segmentation](https://arxiv.org/pdf/2112.01527.pdf) [[笔记]](https://zhuanlan.zhihu.com/p/644070743)

# Survey
* [Milestones in Autonomous Driving and Intelligent Vehicles Survey of Surveys](https://arxiv.org/pdf/2303.17220.pdf)
* [Milestones_in_autonomous_driving_for_intelligent_vehicles_Pt1-2023](https://arxiv.org/pdf/2305.11239.pdf)
* [Milestones in Autonomous Driving and Intelligent Vehicles Part II Perception and Planning](https://arxiv.org/pdf/2306.01980.pdf)
* [Transformer-based models and hardware acceleration analysis in autonomous driving A survey](https://arxiv.org/pdf/2304.10891.pdf)

# Self-supervised
* [Linking vision and motion for self-supervised object-centric perception](https://arxiv.org/pdf/2307.07147.pdf)

# Nerf
* [MapNeRF: Incorporating Map Priors into Neural Radiance Fields for Driving View Simulation](https://arxiv.org/pdf/2307.14981.pdf)[[阅读笔记]](https://zhuanlan.zhihu.com/p/653414617)
* [NeRF-Det: Learning Geometry-Aware Volumetric Representation for Multi-View 3D Object Detection](https://arxiv.org/pdf/2307.14620v1.pdf)
* [Grid-guided Neural Radiance Fields for Large Urban Scenes](https://arxiv.org/pdf/2303.14001.pdf) [[Code]](https://github.com/InternLandMark/LandMark) [[阅读笔记]](https://zhuanlan.zhihu.com/p/647868981)
* [Horizon Robotics: RoMe: Towards Large Scale Road Surface Reconstruction via Mesh Representation](https://arxiv.org/pdf/2306.11368.pdf)
* [Street-View Image Generation from a Bird’s-Eye View Layout](https://arxiv.org/pdf/2301.04634.pdf)
* [BEVControl: Accurately Controlling Street-view Elements with Multi-perspective Consistency via BEV Sketch Layout](https://arxiv.org/pdf/2308.01661.pdf)
* [A Vision-Centric Approach for Static Map Element Annotation](https://arxiv.org/pdf/2309.11754v1.pdf)
* [A Survey on 3D Gaussian Splatting](https://arxiv.org/pdf/2401.03890.pdf)
* [Street Gaussians for Modeling Dynamic Urban Scenes](https://arxiv.org/pdf/2401.01339.pdf)
* [VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction](https://arxiv.org/pdf/2402.17427.pdf)
* [Lightning NeRF: Efficient Hybrid Scene Representation for Autonomous Driving](https://arxiv.org/pdf/2403.05907.pdf)
* [TCLC-GS: Tightly Coupled LiDAR-Camera Gaussian Splatting for Surrounding Autonomous Driving Scenes](https://arxiv.org/pdf/2404.02410.pdf)
* [RoadBEV: Road Surface Reconstruction in Bird’s Eye View](https://arxiv.org/pdf/2404.06605.pdf)
* [DrivingRecon: Large 4D Gaussian Reconstruction Model For Autonomous Driving](https://arxiv.org/pdf/2412.09043)


# Diffusion
* [License Plate Super-Resolution Using Diffusion Models](https://arxiv.org/pdf/2309.12506.pdf)
* [DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving](https://arxiv.org/pdf/2309.09777.pdf)
* [GAIA-1: A Generative World Model for Autonomous Driving](https://arxiv.org/pdf/2309.17080.pdf)
* [Learning Universal Policies via Text-Guided Video Generation](https://arxiv.org/pdf/2302.00111.pdf)
* [Scaling Robot Learning with Semantically Imagined Experience](https://arxiv.org/pdf/2302.11550.pdf)
* [Synthetic Experience Replay](https://openreview.net/pdf?id=0a9p3Ty2k_)
* [Diffusion Model-Augmented Behavioral Cloning](https://arxiv.org/pdf/2302.13335.pdf)
* [Planning with Diffusion for Flexible Behavior Synthesis](https://arxiv.org/pdf/2205.09991.pdf)
* [Is Conditional Generative Modeling all you need for Decision-Making?](https://arxiv.org/pdf/2211.15657.pdf)
* [Imitating Human Behaviour with Diffusion Models](https://arxiv.org/pdf/2301.10677.pdf)
* [Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning](https://arxiv.org/pdf/2208.06193.pdf)
* [Efficient Diffusion Policies for Offline Reinforcement Learning](https://arxiv.org/pdf/2305.20081.pdf)
* [Diffusion Policy: Visuomotor Policy Learning via Action Diffusion](https://arxiv.org/pdf/2303.04137.pdf)
* [Crossway Diffusion: Improving Diffusion-based Visuomotor Policy via Self-supervised Learning](https://arxiv.org/pdf/2307.01849.pdf)
* [A Survey on Video Diffusion Models](https://arxiv.org/pdf/2310.10647.pdf)
* [MagicDrive: Street View Generation with Diverse 3D Geometry Control](https://arxiv.org/pdf/2310.02601.pdf)
* [DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model](https://arxiv.org/pdf/2310.07771.pdf)
* [DrivingGaussian: Composite Gaussian Splatting for Surrounding Dynamic Autonomous Driving Scenes](https://arxiv.org/pdf/2312.07920.pdf)
* [SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis](https://arxiv.org/pdf/2307.01952.pdf)
* [Scalable Diffusion Models with Transformers](https://arxiv.org/pdf/2212.09748.pdf)
* [入门：Tutorial on Diffusion Models for Imaging and Vision](https://arxiv.org/pdf/2403.18103.pdf)
* [DriveArena: A Closed-loop Generative Simulation Platform for Autonomous Driving](https://arxiv.org/pdf/2408.00415)
* [ReconDreamer: Crafting World Models for Driving Scene Reconstruction via Online Restoration](https://arxiv.org/pdf/2411.19548v1)
* [Unraveling the Effects of Synthetic Data on End-to-End Autonomous Driving](https://arxiv.org/pdf/2503.18108)


# CV
* [Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation](https://arxiv.org/pdf/2206.02777.pdf)
* [Segment Everything Everywhere All at Once](https://arxiv.org/pdf/2304.06718.pdf)
* [Multi-modal Queried Object Detection in the Wild](https://arxiv.org/abs/2305.18980)
* [Traffic Sign Interpretation in Real Road Scene](https://arxiv.org/pdf/2311.10793.pdf)
* [Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection](https://arxiv.org/pdf/2303.05499.pdf)
* [Video-GroundingDINO: Towards Open-Vocabulary Spatio-Temporal Video Grounding](https://arxiv.org/pdf/2401.00901.pdf)
* [EfficientSAM Leveraged Masked Image Pretraining for Efficient Segment Anything](https://arxiv.org/pdf/2312.00863.pdf)
* [General Object Foundation Model for Images and Videos at Scale](https://arxiv.org/pdf/2312.09158.pdf)
* [Detecting Everything in the Open World: Towards Universal Object Detection](https://arxiv.org/pdf/2303.11749.pdf)
* [SCLIP: Rethinking Self-Attention for Dense Vision-Language Inference](https://arxiv.org/pdf/2312.01597.pdf)
* [MobileSAMv2: Faster Segment Anything to Everything](https://arxiv.org/pdf/2312.09579.pdf)
* [An Empirical Model of Large-Batch Training](https://arxiv.org/pdf/1812.06162.pdf)
* [Open-Vocabulary SAM: Segment and Recognize Twenty-thousand Classes Interactively](https://arxiv.org/pdf/2401.02955.pdf)
* [360DVD: Controllable Panorama Video Generation with 360-Degree Video Diffusion Model](https://arxiv.org/pdf/2401.06578.pdf)
* [Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data](https://arxiv.org/pdf/2401.10891.pdf)
* [OMG-Seg: Is One Model Good Enough For All Segmentation?](https://arxiv.org/pdf/2401.10229.pdf)
* [EfficientViT-SAM: Accelerated Segment Anything Model Without Performance Loss](https://arxiv.org/pdf/2402.05008.pdf)
* [UniMODE: Unified Monocular 3D Object Detection](https://arxiv.org/pdf/2402.18573.pdf)
* [LightM-UNet: Mamba Assists in Lightweight UNet for Medical Image Segmentation](https://arxiv.org/pdf/2403.05246.pdf)
* [Adapting LLaMA Decoder to Vision Transformer](https://arxiv.org/pdf/2404.06773.pdf)
* [An Introduction to Vision-Language Modeling](https://arxiv.org/pdf/2405.17247)
* [D-FINE: Redefine Regression Task of DETRs as Fine-grained Distribution Refinement](https://arxiv.org/pdf/2410.13842)
* [FAST-LIVO2: Fast, Direct LiDAR-Inertial-Visual Odometry](https://arxiv.org/pdf/2408.14035)
* [DETRs Beat YOLOs on Real-time Object Detection](https://arxiv.org/pdf/2304.08069)
* [Group DETR: Fast DETR Training with Group-Wise One-to-Many Assignment](https://arxiv.org/pdf/2207.13085)



# End-to-End
* [Survey: End-to-end Autonomous Driving: Challenges and Frontiers](https://arxiv.org/pdf/2306.16927.pdf)[[阅读笔记]](https://zhuanlan.zhihu.com/p/645591804/edit)
* [Survey: Recent Advancements in End-to-End Autonomous Driving using Deep Learning: A Survey](https://arxiv.org/pdf/2307.04370.pdf)
* [Github: End-to-end Autonomous Driving](https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving)
* [POLICY PRE-TRAINING FOR AUTONOMOUS DRIVING VIA SELF-SUPERVISED GEOMETRIC MODELING](https://arxiv.org/pdf/2301.01006.pdf)
* [World Models](https://arxiv.org/abs/1803.10122)
* [SafetyNet: Safe planning for real-world self-driving vehicles using machine-learned policies](https://arxiv.org/pdf/2109.13602.pdf)
* [UniAD: Goal-oriented Autonomous Driving](https://github.com/OpenDriveLab/UniAD)
* [DriveAdapter: Breaking the Coupling Barrier of Perception and Planning in End-to-End Autonomous Driving](https://arxiv.org/pdf/2308.00398.pdf)
* [FusionAD: Multi-modality Fusion for Prediction and Planning Tasks of Autonomous Driving](https://arxiv.org/pdf/2308.01006.pdf) [[Code]](https://github.com/westlake-autolab/FusionAD)
* [Interpretable End-to-End Driving Model for Implicit Scene Understanding](https://arxiv.org/pdf/2308.01180.pdf)
* [GPT-DRIVER: LEARNING TO DRIVE WITH GPT](https://arxiv.org/pdf/2310.01415.pdf) [[阅读笔记]](https://zhuanlan.zhihu.com/p/660803092)
* [Perceive, Interact, Predict: Learning Dynamic and Static Clues for End-to-End Motion Prediction](https://arxiv.org/pdf/2212.02181.pdf)
* [VAD Vectorized Scene Representation for Efficient Autonomous Driving](https://arxiv.org/pdf/2303.12077.pdf)
* [ViP3D: End-to-end Visual Trajectory Prediction via 3D Agent Queries](https://arxiv.org/pdf/2208.01582.pdf)
* [MMFN: Multi-Modal-Fusion-Net for End-to-End Driving](https://arxiv.org/pdf/2207.00186.pdf)
* [Exploring Navigation Maps for Learning-Based Motion Prediction](https://arxiv.org/pdf/2302.06195.pdf)
* [ST-P3 End-to-end Vision-based Autonomous Driving via Spatial-Temporal Feature Learning](https://arxiv.org/pdf/2207.07601.pdf)
* [MP3: A Unified Model to Map, Perceive, Predict and Plan](https://arxiv.org/pdf/2101.06806.pdf)
* [Is Ego Status All You Need for Open-Loop End-to-End Autonomous Driving?](https://arxiv.org/pdf/2312.03031.pdf)
* [A Survey for Foundation Models in Autonomous Driving](https://arxiv.org/pdf/2402.01105.pdf)
* [Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving](https://arxiv.org/pdf/2311.17918.pdf)
* [ActiveAD: Planning-Oriented Active Learning for End-to-End Autonomous Driving](https://arxiv.org/pdf/2403.02877.pdf)
* [Driving in the Occupancy World: Vision-Centric 4D Occupancy Forecasting and Planning via World Models for Autonomous Driving](https://arxiv.org/pdf/2408.14197v1)
* [NoMaD: Goal Masked Diffusion Policies for Navigation and Exploration](https://arxiv.org/pdf/2310.07896)
* [PPAD: Iterative Interactions of Prediction and Planning for End-to-end Autonomous Driving](https://arxiv.org/pdf/2311.08100)
* [Joint Perception and Prediction for Autonomous Driving: A Survey](https://arxiv.org/pdf/2412.14088)
* [CarPlanner: Consistent Auto-regressive Trajectory Planning for Large-scale Reinforcement Learning in Autonomous Driving](https://arxiv.org/pdf/2502.19908)
* [VDT-Auto: End-to-end Autonomous Driving with VLM-Guided Diffusion Transformers](https://arxiv.org/pdf/2502.20108)
* [VLM-E2E: Enhancing End-to-End Autonomous Driving with Multimodal Driver Attention Fusion](https://arxiv.org/pdf/2502.18042)
* [VaViM and VaVAM: Autonomous Driving through Video Generative Modeling](https://arxiv.org/pdf/2502.15672)
* [Sce2DriveX: A Generalized MLLM Framework for Scene-to-Drive Learning](https://arxiv.org/pdf/2502.14917)
* [DriveTransformer: Unified Transformer for Scalable End-to-End Autonomous Driving](https://openreview.net/pdf?id=M42KR4W9P5)
* [GoalFlow: Goal-Driven Flow Matching for Multimodal Trajectories Generation in End-to-End Autonomous Driving](https://arxiv.org/pdf/2503.05689)
* [AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning](https://arxiv.org/pdf/2503.07608)
* [CATPlan: Loss-based Collision Prediction in End-to-End Autonomous Driving](https://arxiv.org/pdf/2503.07425)


# Robotic
* [Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion](https://arxiv.org/pdf/2407.10973)
* [Surfer: Progressive Reasoning with World Models for Robotic Manipulation](https://arxiv.org/pdf/2306.11335)
* [End-to-End Navigation with Vision-Language Models: Transforming Spatial Reasoning into Question-Answering](https://arxiv.org/pdf/2411.05755v1)
* [NaVILA: Legged Robot Vision-Language-Action Model for Navigation](https://arxiv.org/pdf/2412.04453v1)
* [Uni-NaVid: A Video-based Vision-Language-Action Model for Unifying Embodied Navigation Tasks](https://arxiv.org/pdf/2412.06224)



# LLM and AD
* [A Survey of Large Language Models for Autonomous Driving](https://arxiv.org/pdf/2311.01043.pdf) 
* [A Survey on Multimodal Large Language Models for Autonomous Driving](https://arxiv.org/pdf/2311.12320.pdf)
* [https://github.com/Thinklab-SJTU/Awesome-LLM4AD](https://github.com/Thinklab-SJTU/Awesome-LLM4AD)
* [DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model](https://arxiv.org/pdf/2310.01412.pdf)
* [ADAPT: Action-aware Driving Caption Transformer](https://arxiv.org/pdf/2302.00673.pdf)
* [Driving with LLMs Fusing Object-Level Vector Modality for Explainable Autonomous Driving](https://arxiv.org/pdf/2310.01957v1.pdf)
* [Language Prompt for Autonomous Driving](https://arxiv.org/pdf/2309.04379.pdf)
* [On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving](https://arxiv.org/pdf/2311.05332.pdf)
* [Vision Language Models in Autonomous Driving and Intelligent Transportation Systems](https://arxiv.org/pdf/2310.14414.pdf)
* [Bird’s-Eye-View Scene Graph for Vision-Language Navigation](https://arxiv.org/pdf/2308.04758.pdf) [【阅读笔记】](?)
* [Waymax: An Accelerated, Data-Driven Simulator for Large-Scale Autonomous Driving Research](https://arxiv.org/pdf/2310.08710v1.pdf)
* [BEVGPT: Generative Pre-trained Large Model for Autonomous Driving Prediction, Decision-Making, and Planning](https://arxiv.org/pdf/2310.10357.pdf)
* [UniPAD: A Universal Pre-training Paradigm for Autonomous Driving](https://arxiv.org/pdf/2310.08370.pdf)
* [A Practical Large-Scale Roadside Multi-View Multi-Sensor Spatial Synchronization Framework for Intelligent Transportation Systems](https://arxiv.org/pdf/2311.04231.pdf)
* [A Language Agent for Autonomous Driving](https://arxiv.org/pdf/2311.10813.pdf)
* [ADriver-I: A General World Model for Autonomous Driving](https://arxiv.org/pdf/2311.13549.pdf)
* [Panacea: Panoramic and Controllable Video Generation for Autonomous Driving](https://arxiv.org/pdf/2311.16813.pdf)
* [Reason2Drive: Towards Interpretable and Chain-based Reasoning for Autonomous Driving](https://arxiv.org/pdf/2312.03661.pdf)
* [Towards Knowledge-driven Autonomous Driving](https://arxiv.org/pdf/2312.04316.pdf)
* [DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving](https://arxiv.org/pdf/2312.09245.pdf)
* [VLP: Vision Language Planning for Autonomous Driving](https://arxiv.org/pdf/2401.05577.pdf)
* [Forging Vision Foundation Models for Autonomous Driving: Challenges, Methodologies, and Opportunities](https://arxiv.org/pdf/2401.08045.pdf)
* [RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model](https://arxiv.org/pdf/2402.10828.pdf)
* [DRIVEVLM: The Convergence of Autonomous Driving and Large Vision-Language Models](https://arxiv.org/pdf/2402.12289.pdf)
* [GenAD: Generative End-to-End Autonomous Driving](https://arxiv.org/pdf/2402.11502.pdf)
* [VADv2: End-to-End Vectorized Autonomous Driving via Probabilistic Planning](https://arxiv.org/pdf/2402.13243.pdf)
* [Embodied Understanding of Driving Scenarios](https://arxiv.org/pdf/2403.04593.pdf)
* [DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation](https://arxiv.org/pdf/2403.06845.pdf)
* [Generalized Predictive Model for Autonomous Driving](https://arxiv.org/pdf/2403.09630.pdf)
* [SparseAD: Sparse Query-Centric Paradigm for Efficient End-to-End Autonomous Driving](https://arxiv.org/pdf/2404.06892.pdf)
* [OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning and Planning](https://arxiv.org/pdf/2405.01533)
* [Hint-AD: Holistically Aligned Interpretability in End-to-End Autonomous Driving](https://arxiv.org/pdf/2409.06702)
* [Video Token Sparsification for Efficient Multimodal LLMs in Autonomous Driving](https://arxiv.org/pdf/2409.11182v1)
* [☆][HE-Drive: Human-Like End-to-End Driving with Vision Language Models](https://arxiv.org/pdf/2410.05051)
* [Senna: Bridging Large Vision-Language Models and End-to-End Autonomous Driving](https://arxiv.org/pdf/2410.22313)
* [EMMA: End-to-End Multimodal Modelfor Autonomous Driving](https://arxiv.org/pdf/2410.23262)
* [☆][DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving](https://arxiv.org/pdf/2411.15139v1)
* [VisionPAD: A Vision-Centric Pre-training Paradigm for Autonomous Driving](https://arxiv.org/pdf/2411.14716v1)
* [DriveMM: All-in-One Large Multimodal Model for Autonomous Driving](https://arxiv.org/pdf/2412.07689v1)
* [Doe-1: Closed-Loop Autonomous Driving with Large World Model](https://arxiv.org/pdf/2412.09627)
* [WiseAD: Knowledge Augmented End-to-End Autonomous Driving with Vision-Language Model](https://arxiv.org/pdf/2412.09951v1)
* [☆][Diffusion-Based Planning for Autonomous Driving with Flexible Guidance](https://arxiv.org/pdf/2501.15564)
* [☆][DiffAD: A Unified Diffusion Modeling Approach for Autonomous Driving](https://arxiv.org/pdf/2503.12170)
* [☆][Hydra-NeXt: Robust Closed-Loop Driving with Open-Loop Training](https://arxiv.org/pdf/2503.12030)

* [Unraveling the Effects of Synthetic Data on End-to-End Autonomous Driving](https://arxiv.org/pdf/2503.18108)
* [ReconDreamer++: Harmonizing Generative and Reconstructive Models for Driving Scene Representation](https://arxiv.org/pdf/2503.18438)
* [DiST-4D: Disentangled Spatiotemporal Diffusion with Metric Depth for 4D Driving Scene Generation](https://arxiv.org/pdf/2503.15208)
* [MiLA: Multi-view Intensive-fidelity Long-term Video Generation World Model for Autonomous Driving](https://arxiv.org/pdf/2503.15875)
* [NuPlanQA: A Large-Scale Dataset and Benchmark for Multi-View Driving Scene Understanding in Multi-Modal Large Language Models](https://arxiv.org/pdf/2503.12772)
* [Centaur: Robust End-to-End Autonomous Driving with Test-Time Training](https://arxiv.org/pdf/2503.11650v1)
* [GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving](https://arxiv.org/pdf/2503.20523)
* [ORION: A Holistic End-to-End Autonomous Driving Framework by Vision-Language Instructed Action Generation](https://arxiv.org/pdf/2503.19755) [可以用千万VL中固定个数query替换qformer]
* [π0.5: a Vision-Language-Action Model with Open-World Generalization](https://arxiv.org/pdf/2504.16054)
* [☆☆][Robust Autonomy Emerges from Self-Play](https://arxiv.org/pdf/2502.03349) [1. GIGAFLOW模拟器：高效批量模拟，支持每秒数亿次状态转换，成本低于5美元/百万公里。 2.通用策略（Generalist Policy）：单一策略适配多类交通参与者（汽车、卡车、行人等）和驾驶风格，零样本泛化至多个基准测试。3.训练规模：总计1.6亿公里驾驶数据（9500年经验），远超以往研究。  ]
