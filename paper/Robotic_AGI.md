# End-to-End
* [Survey: End-to-end Autonomous Driving: Challenges and Frontiers](https://arxiv.org/pdf/2306.16927.pdf)[[阅读笔记]](https://zhuanlan.zhihu.com/p/645591804/edit)
* [Survey: Recent Advancements in End-to-End Autonomous Driving using Deep Learning: A Survey](https://arxiv.org/pdf/2307.04370.pdf)
* [Github: End-to-end Autonomous Driving](https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving)
* [POLICY PRE-TRAINING FOR AUTONOMOUS DRIVING VIA SELF-SUPERVISED GEOMETRIC MODELING](https://arxiv.org/pdf/2301.01006.pdf)
* [World Models](https://arxiv.org/abs/1803.10122)
* [SafetyNet: Safe planning for real-world self-driving vehicles using machine-learned policies](https://arxiv.org/pdf/2109.13602.pdf)
* [UniAD: Goal-oriented Autonomous Driving](https://github.com/OpenDriveLab/UniAD)
* [DriveAdapter: Breaking the Coupling Barrier of Perception and Planning in End-to-End Autonomous Driving](https://arxiv.org/pdf/2308.00398.pdf)
* [FusionAD: Multi-modality Fusion for Prediction and Planning Tasks of Autonomous Driving](https://arxiv.org/pdf/2308.01006.pdf) [[Code]](https://github.com/westlake-autolab/FusionAD)
* [Interpretable End-to-End Driving Model for Implicit Scene Understanding](https://arxiv.org/pdf/2308.01180.pdf)
* [GPT-DRIVER: LEARNING TO DRIVE WITH GPT](https://arxiv.org/pdf/2310.01415.pdf) [[阅读笔记]](https://zhuanlan.zhihu.com/p/660803092)
* [Perceive, Interact, Predict: Learning Dynamic and Static Clues for End-to-End Motion Prediction](https://arxiv.org/pdf/2212.02181.pdf)
* [VAD Vectorized Scene Representation for Efficient Autonomous Driving](https://arxiv.org/pdf/2303.12077.pdf)
* [ViP3D: End-to-end Visual Trajectory Prediction via 3D Agent Queries](https://arxiv.org/pdf/2208.01582.pdf)
* [MMFN: Multi-Modal-Fusion-Net for End-to-End Driving](https://arxiv.org/pdf/2207.00186.pdf)
* [Exploring Navigation Maps for Learning-Based Motion Prediction](https://arxiv.org/pdf/2302.06195.pdf)
* [ST-P3 End-to-end Vision-based Autonomous Driving via Spatial-Temporal Feature Learning](https://arxiv.org/pdf/2207.07601.pdf)
* [MP3: A Unified Model to Map, Perceive, Predict and Plan](https://arxiv.org/pdf/2101.06806.pdf)
* [Is Ego Status All You Need for Open-Loop End-to-End Autonomous Driving?](https://arxiv.org/pdf/2312.03031.pdf)
* [A Survey for Foundation Models in Autonomous Driving](https://arxiv.org/pdf/2402.01105.pdf)
* [Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving](https://arxiv.org/pdf/2311.17918.pdf)
* [ActiveAD: Planning-Oriented Active Learning for End-to-End Autonomous Driving](https://arxiv.org/pdf/2403.02877.pdf)
* [Driving in the Occupancy World: Vision-Centric 4D Occupancy Forecasting and Planning via World Models for Autonomous Driving](https://arxiv.org/pdf/2408.14197v1)
* [NoMaD: Goal Masked Diffusion Policies for Navigation and Exploration](https://arxiv.org/pdf/2310.07896)
* [PPAD: Iterative Interactions of Prediction and Planning for End-to-end Autonomous Driving](https://arxiv.org/pdf/2311.08100)



# Robotic
* [Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion](https://arxiv.org/pdf/2407.10973)
* [Surfer: Progressive Reasoning with World Models for Robotic Manipulation](https://arxiv.org/pdf/2306.11335)

# LLM and AD
* [A Survey of Large Language Models for Autonomous Driving](https://arxiv.org/pdf/2311.01043.pdf) 
* [A Survey on Multimodal Large Language Models for Autonomous Driving](https://arxiv.org/pdf/2311.12320.pdf)
* [https://github.com/Thinklab-SJTU/Awesome-LLM4AD](https://github.com/Thinklab-SJTU/Awesome-LLM4AD)
* [DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model](https://arxiv.org/pdf/2310.01412.pdf)
* [ADAPT: Action-aware Driving Caption Transformer](https://arxiv.org/pdf/2302.00673.pdf)
* [Driving with LLMs Fusing Object-Level Vector Modality for Explainable Autonomous Driving](https://arxiv.org/pdf/2310.01957v1.pdf)
* [Language Prompt for Autonomous Driving](https://arxiv.org/pdf/2309.04379.pdf)
* [On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving](https://arxiv.org/pdf/2311.05332.pdf)
* [Vision Language Models in Autonomous Driving and Intelligent Transportation Systems](https://arxiv.org/pdf/2310.14414.pdf)
* [Bird’s-Eye-View Scene Graph for Vision-Language Navigation](https://arxiv.org/pdf/2308.04758.pdf) [【阅读笔记】](?)
* [Waymax: An Accelerated, Data-Driven Simulator for Large-Scale Autonomous Driving Research](https://arxiv.org/pdf/2310.08710v1.pdf)
* [BEVGPT: Generative Pre-trained Large Model for Autonomous Driving Prediction, Decision-Making, and Planning](https://arxiv.org/pdf/2310.10357.pdf)
* [UniPAD: A Universal Pre-training Paradigm for Autonomous Driving](https://arxiv.org/pdf/2310.08370.pdf)
* [A Practical Large-Scale Roadside Multi-View Multi-Sensor Spatial Synchronization Framework for Intelligent Transportation Systems](https://arxiv.org/pdf/2311.04231.pdf)
* [A Language Agent for Autonomous Driving](https://arxiv.org/pdf/2311.10813.pdf)
* [ADriver-I: A General World Model for Autonomous Driving](https://arxiv.org/pdf/2311.13549.pdf)
* [Panacea: Panoramic and Controllable Video Generation for Autonomous Driving](https://arxiv.org/pdf/2311.16813.pdf)
* [Reason2Drive: Towards Interpretable and Chain-based Reasoning for Autonomous Driving](https://arxiv.org/pdf/2312.03661.pdf)
* [Towards Knowledge-driven Autonomous Driving](https://arxiv.org/pdf/2312.04316.pdf)
* [DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving](https://arxiv.org/pdf/2312.09245.pdf)
* [VLP: Vision Language Planning for Autonomous Driving](https://arxiv.org/pdf/2401.05577.pdf)
* [Forging Vision Foundation Models for Autonomous Driving: Challenges, Methodologies, and Opportunities](https://arxiv.org/pdf/2401.08045.pdf)
* [RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model](https://arxiv.org/pdf/2402.10828.pdf)
* [DRIVEVLM: The Convergence of Autonomous Driving and Large Vision-Language Models](https://arxiv.org/pdf/2402.12289.pdf)
* [GenAD: Generative End-to-End Autonomous Driving](https://arxiv.org/pdf/2402.11502.pdf)
* [VADv2: End-to-End Vectorized Autonomous Driving via Probabilistic Planning](https://arxiv.org/pdf/2402.13243.pdf)
* [Embodied Understanding of Driving Scenarios](https://arxiv.org/pdf/2403.04593.pdf)
* [DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation](https://arxiv.org/pdf/2403.06845.pdf)
* [Generalized Predictive Model for Autonomous Driving](https://arxiv.org/pdf/2403.09630.pdf)
* [SparseAD: Sparse Query-Centric Paradigm for Efficient End-to-End Autonomous Driving](https://arxiv.org/pdf/2404.06892.pdf)
* [OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning and Planning](https://arxiv.org/pdf/2405.01533)
* [Hint-AD: Holistically Aligned Interpretability in End-to-End Autonomous Driving](https://arxiv.org/pdf/2409.06702)
* [Video Token Sparsification for Efficient Multimodal LLMs in Autonomous Driving](https://arxiv.org/pdf/2409.11182v1)
* [HE-Drive: Human-Like End-to-End Driving with Vision Language Models](https://arxiv.org/pdf/2410.05051)