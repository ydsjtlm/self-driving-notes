# BEV检测
* [Fast-BEV: A Fast and Strong Bird's-Eye View Perception Baseline](https://arxiv.org/abs/2301.12511) [[Code]](https://github.com/Sense-GVT/Fast-BEV) [[资料]](https://mp.weixin.qq.com/s/Dv8UVrRN2TrOTQCqVtDSJg)
* [M2BEV: Multi-Camera Joint 3D Detection and Segmentation with Unified Bird’s-Eye View Representation](https://arxiv.org/pdf/2204.05088.pdf) [[阅读笔记]](https://zhuanlan.zhihu.com/p/651960376)
* [MatrixVT: Efficient Multi-Camera to BEV Transformation for 3D Perception](https://arxiv.org/abs/2211.10593)
* [BEV-IO: Enhancing Bird’s-Eye-View 3D Detection with Instance Occupancy](https://arxiv.org/pdf/2305.16829.pdf)
* [PETRv2: A Unified Framework for 3D Perception from Multi-Camera Images](https://arxiv.org/pdf/2206.01256.pdf)
* [Exploring Object-Centric Temporal Modeling for Efficient Multi-View 3D Object Detection](https://arxiv.org/pdf/2303.11926.pdf)
* [OCBEV Object-Centric BEV Transformer for Multi-View 3D Object Detection](https://arxiv.org/pdf/2306.01738.pdf)
* [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation](https://arxiv.org/pdf/2308.09244.pdf)
* [MonoNeRD: NeRF-like Representations for Monocular 3D Object Detection](https://arxiv.org/pdf/2308.09421.pdf)
* [Temporal Enhanced Training of Multi-view 3D Object Detector via Historical Object Prediction](https://arxiv.org/pdf/2304.00967.pdf)
* [DFA3D: 3D Deformable Attention For 2D-to-3D Feature Lifting](https://arxiv.org/pdf/2307.12972.pdf)
* [EA-LSS: Edge-aware Lift-splat-shot Framework for 3D BEV Object Detection](https://arxiv.org/abs/2303.17895)

# BEV分割 (LaneGraph)
* [Learning and Aggregating Lane Graphs for Urban Automated Driving](https://arxiv.org/abs/2302.06175) 
* [InstaGraM Instance-level Graph Modeling for Vectorized HD Map Learning](https://arxiv.org/abs/2301.04470)
* [(2021) Structured Bird’s-Eye-View Traffic Scene Understanding from Onboard Images](https://arxiv.org/pdf/2110.01997.pdf)
* [(2022) Topology Preserving Local Road Network Estimation from Single Onboard Camera Image](https://openaccess.thecvf.com/content/CVPR2022/papers/Can_Topology_Preserving_Local_Road_Network_Estimation_From_Single_Onboard_Camera_CVPR_2022_paper.pdf)
* [(2022) Lane-Level Street Map Extraction from Aerial Imagery](https://openaccess.thecvf.com/content/WACV2022/papers/He_Lane-Level_Street_Map_Extraction_From_Aerial_Imagery_WACV_2022_paper.pdf)
* [(2023) Lane Graph as Path: Continuity-preserving Path-wise Modeling for Online Lane Graph Construction](https://github.com/hustvl/LaneGAP)
* [MapTR: Structured Modeling and Learning for Online Vectorized HD Map Construction](https://arxiv.org/pdf/2208.14437.pdf) [[Code]](https://github.com/hustvl/MapTR) [[笔记]](https://)
* [Neural Map Prior for Autonomous Driving](https://arxiv.org/pdf/2304.08481.pdf)
* [VMA: Divide-and-Conquer Vectorized Map Annotation System for Large-Scale Driving Scene](https://arxiv.org/pdf/2304.09807.pdf)
* [End-to-End Vectorized HD-map Construction with Piecewise Bezier Curve](https://openaccess.thecvf.com/content/CVPR2023/papers/Qiao_End-to-End_Vectorized_HD-Map_Construction_With_Piecewise_Bezier_Curve_CVPR_2023_paper.pdf)
* [MachMap: End-to-End Vectorized Solution for Compact HD-Map Construction](https://arxiv.org/pdf/2306.10301.pdf)
* [MV-Map: Offboard HD-Map Generation with Multi-view Consistency](https://arxiv.org/pdf/2305.08851.pdf)
* [LATR: 3D Lane Detection from Monocular Images with Transformer](https://arxiv.org/pdf/2308.04583.pdf)
* [MapTRv2: An End-to-End Framework for Online Vectorized HD Map Construction](https://arxiv.org/pdf/2308.05736v1.pdf)
* [InsightMapper: A Closer Look at Inner-instance Information for Vectorized High-Definition Mapping](https://arxiv.org/pdf/2308.08543.pdf)[[Code]]() [[笔记]](https://zhuanlan.zhihu.com/p/650732045)
* [StreamMapNet: Streaming Mapping Network for Vectorized Online HD Map Construction](https://arxiv.org/pdf/2308.12570.pdf)
* HDmapNet VectorNetMap

# BEV Occupancy
* [Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction](https://arxiv.org/pdf/2302.07817.pdf) [[Code]](https://github.com/wzzheng/TPVFormer) [[笔记]](https://zhuanlan.zhihu.com/p/648813002)
* [MonoScene Monocular 3D Semantic Scene Completion](https://arxiv.org/abs/2112.00726)
* [OccDepth: A Depth-aware Method for 3D Semantic Occupancy Network](https://arxiv.org/abs/2302.13540)
* [VoxFormer: Sparse Voxel Transformer for Camera-based 3D Semantic Scene Completion](https://arxiv.org/abs/2302.12251)
* [OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception](https://github.com/JeffWang987/OpenOccupancy)
* [A Simple Attempt for 3D Occupancy Estimation in Autonomous Driving](https://github.com/GANWANSHUI/SimpleOccupancy)
* [Occ-BEV: Multi-Camera Unified Pre-training via 3D Scene Reconstruction](https://arxiv.org/pdf/2305.18829.pdf)
* [Learning Occupancy for Monocular 3D Object Detection](https://arxiv.org/pdf/2305.15694.pdf)
* [Scene as Occupancy](https://arxiv.org/pdf/2306.02851.pdf) [[Code]](https://github.com/OpenDriveLab/OccNet)
* [UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering](https://arxiv.org/pdf/2306.09117.pdf)
* [FB-OCC 3D Occupancy Prediction based on Forward-Backward View Transformation](https://opendrivelab.com/e2ead/AD23Challenge/Track_3_NVOCC.pdf?=&linkId=100000205404832)[[Code]](https://github.com/NVlabs/FB-BEV) [【阅读笔记】](https://zhuanlan.zhihu.com/p/642730470)
* [OVO: Open-Vocabulary Occupancy](https://arxiv.org/pdf/2305.16133.pdf)
* [OCTraN: 3D Occupancy Convolutional Transformer Network in Unstructured Traffic Scenarios](https://arxiv.org/pdf/2307.10934.pdf)

# 矢量化
* [PolyFormer Referring Image Segmentation as Sequential Polygon Generation](https://arxiv.org/abs/2302.07387)[[阅读笔记]](https://zhuanlan.zhihu.com/p/650991896)


# 多传感器融合
* [Transformers-Sensor-Fusion](https://github.com/ApoorvRoboticist/Transformers-Sensor-Fusion)
* [UniTR: A Unified and Efficient Multi-Modal Transformer for Bird’s-Eye-View Representation]()

# 检测跟踪一体化
* [Exploring Simple 3D Multi-Object Tracking for Autonomous Driving](https://github.com/qcraftai/simtrack)
* [MOTR: End-to-End Multiple-Object Tracking with TRansformer](https://github.com/megvii-research/MOTR)
* [MUTR3D: A Multi-camera Tracking Framework via 3D-to-2D Queries](https://tsinghua-mars-lab.github.io/mutr3d/)
* [MotionTrack: End-to-End Transformer-based Multi-Object Tracing with LiDAR-Camera Fusion](https://arxiv.org/pdf/2306.17000.pdf)


# End-to-End
* [Github: End-to-end Autonomous Driving](https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving)
* [POLICY PRE-TRAINING FOR AUTONOMOUS DRIVING VIA SELF-SUPERVISED GEOMETRIC MODELING](https://arxiv.org/pdf/2301.01006.pdf)
* [World Models](https://arxiv.org/abs/1803.10122)
* [SafetyNet: Safe planning for real-world self-driving vehicles using machine-learned policies](https://arxiv.org/pdf/2109.13602.pdf)
* [Goal-oriented Autonomous Driving](https://github.com/OpenDriveLab/UniAD)
* [综述: End-to-end Autonomous Driving: Challenges and Frontiers](https://arxiv.org/pdf/2306.16927.pdf)[[阅读笔记]](https://zhuanlan.zhihu.com/p/645591804/edit)
* [综述: Recent Advancements in End-to-End Autonomous Driving using Deep Learning: A Survey](https://arxiv.org/pdf/2307.04370.pdf)
* [DriveAdapter: Breaking the Coupling Barrier of Perception and Planning in End-to-End Autonomous Driving](https://arxiv.org/pdf/2308.00398.pdf)
* [FusionAD: Multi-modality Fusion for Prediction and Planning Tasks of Autonomous Driving](https://arxiv.org/pdf/2308.01006.pdf) [[Code]](https://github.com/westlake-autolab/FusionAD)
* [Interpretable End-to-End Driving Model for Implicit Scene Understanding](https://arxiv.org/pdf/2308.01180.pdf)

# Prediction
* [Motion Transformer with Global Intention Localization and Local Movement Refinement](https://arxiv.org/pdf/2209.13508.pdf)


# Calibration
* [☆] [Calib-Anything: Zero-training LiDAR-Camera Extrinsic Calibration Method Using Segment Anything](https://arxiv.org/pdf/2306.02656.pdf)

# Simulation 
* [UniSim: A Neural Closed-Loop Sensor Simulator](https://waabi.ai/wp-content/uploads/2023/05/UniSim-paper.pdf)
* [Rethinking Closed-loop Training for Autonomous Driving](https://arxiv.org/pdf/2306.15713.pdf)


# Transformer
* [SegViTv2: Exploring Efficient and Continual Semantic Segmentation with Plain Vision Transformers](https://arxiv.org/pdf/2306.06289.pdf)
* [Taming Transformers for High-Resolution Image Synthesis](https://arxiv.org/pdf/2012.09841.pdf)
* [Mask2Former: Masked-attention Mask Transformer for Universal Image Segmentation](https://arxiv.org/pdf/2112.01527.pdf) [[笔记]](https://zhuanlan.zhihu.com/p/644070743)

# Survey
* [Milestones in Autonomous Driving and Intelligent Vehicles Survey of Surveys](https://arxiv.org/pdf/2303.17220.pdf)
* [Milestones_in_autonomous_driving_for_intelligent_vehicles_Pt1-2023](https://arxiv.org/pdf/2305.11239.pdf)
* [Milestones in Autonomous Driving and Intelligent Vehicles Part II Perception and Planning](https://arxiv.org/pdf/2306.01980.pdf)
* [Transformer-based models and hardware acceleration analysis in autonomous driving A survey](https://arxiv.org/pdf/2304.10891.pdf)

# Self-supervised
* [Linking vision and motion for self-supervised object-centric perception](https://arxiv.org/pdf/2307.07147.pdf)

# Nerf
* [MapNeRF: Incorporating Map Priors into Neural Radiance Fields for Driving View Simulation](https://arxiv.org/pdf/2307.14981.pdf)
* [NeRF-Det: Learning Geometry-Aware Volumetric Representation for Multi-View 3D Object Detection](https://arxiv.org/pdf/2307.14620v1.pdf)
* [Grid-guided Neural Radiance Fields for Large Urban Scenes](https://arxiv.org/pdf/2303.14001.pdf) [[Code]](https://github.com/InternLandMark/LandMark) [[阅读笔记]](https://zhuanlan.zhihu.com/p/647868981)
* [Horizon Robotics: RoMe: Towards Large Scale Road Surface Reconstruction via Mesh Representation](https://arxiv.org/pdf/2306.11368.pdf)
* [Street-View Image Generation from a Bird’s-Eye View Layout](https://arxiv.org/pdf/2301.04634.pdf)
* [BEVControl: Accurately Controlling Street-view Elements with Multi-perspective Consistency via BEV Sketch Layout](https://arxiv.org/pdf/2308.01661.pdf)
