# BEV检测
* [Fast-BEV: A Fast and Strong Bird's-Eye View Perception Baseline](https://arxiv.org/abs/2301.12511) [[Code]](https://github.com/Sense-GVT/Fast-BEV) [[资料]](https://mp.weixin.qq.com/s/Dv8UVrRN2TrOTQCqVtDSJg)
* [M2BEV: Multi-Camera Joint 3D Detection and Segmentation with Unified Bird’s-Eye View Representation](https://arxiv.org/pdf/2204.05088.pdf) [[阅读笔记]](https://zhuanlan.zhihu.com/p/651960376)
* [MatrixVT: Efficient Multi-Camera to BEV Transformation for 3D Perception](https://arxiv.org/abs/2211.10593)
* [BEV-IO: Enhancing Bird’s-Eye-View 3D Detection with Instance Occupancy](https://arxiv.org/pdf/2305.16829.pdf)
* [PETRv2: A Unified Framework for 3D Perception from Multi-Camera Images](https://arxiv.org/pdf/2206.01256.pdf)
* [StreamPetr: Exploring Object-Centric Temporal Modeling for Efficient Multi-View 3D Object Detection](https://arxiv.org/pdf/2303.11926.pdf)
* [OCBEV Object-Centric BEV Transformer for Multi-View 3D Object Detection](https://arxiv.org/pdf/2306.01738.pdf)
* [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation](https://arxiv.org/pdf/2308.09244.pdf)
* [MonoNeRD: NeRF-like Representations for Monocular 3D Object Detection](https://arxiv.org/pdf/2308.09421.pdf)
* [Temporal Enhanced Training of Multi-view 3D Object Detector via Historical Object Prediction](https://arxiv.org/pdf/2304.00967.pdf)
* [DFA3D: 3D Deformable Attention For 2D-to-3D Feature Lifting](https://arxiv.org/pdf/2307.12972.pdf)
* [EA-LSS: Edge-aware Lift-splat-shot Framework for 3D BEV Object Detection](https://arxiv.org/abs/2303.17895)
* [Cross Modal Transformer: Towards Fast and Robust 3D Object Detection](https://arxiv.org/pdf/2301.01283.pdf)
* [BEVHeight++: Toward Robust Visual Centric 3D Object Detection](https://arxiv.org/pdf/2309.16179.pdf)
* [DistillBEV: Boosting Multi-Camera 3D Object Detection with Cross-Modal Knowledge Distillation](https://browse.arxiv.org/pdf/2309.15109.pdf)
* [DynamicBEV: Leveraging Dynamic Queries and Temporal Context for 3D Object Detection.](https://openreview.net/pdf?id=rljudc4XHW)
* [Leveraging Vision-Centric Multi-Modal Expertise for 3D Object Detection](https://arxiv.org/pdf/2310.15670.pdf)
* [BEVStereo: Enhancing Depth Estimation in Multi-view 3D Object Detection with Dynamic Temporal Stereo](https://arxiv.org/pdf/2209.10248.pdf)
* [SOLOFusion Time Will Tell New Outlooks and A Baseline for Temporal Multi-View 3D Object Detection](https://arxiv.org/pdf/2210.02443.pdf)
* [VideoBEV_Exploring Recurrent Long-term Temporal Fusion for Multi-view 3D Perception](https://arxiv.org/pdf/2303.05970.pdf)
* [Long Range: Far3D: Expanding the Horizon for Surround-view 3D Object Detection](https://arxiv.org/pdf/2308.09616.pdf)
* [Sparse4D: Multi-view 3D Object Detection with Sparse Spatial-Temporal Fusion](https://arxiv.org/pdf/2211.10581.pdf)
* [Sparse4D v2: Recurrent Temporal Fusion with Sparse Model](https://arxiv.org/pdf/2305.14018.pdf)
* [Sparse4D v3: Advancing End-to-End 3D Detection and Tracking](https://arxiv.org/pdf/2311.11722.pdf)
* [Towards Efficient 3D Object Detection in Bird’s-Eye-View Space for Autonomous Driving: A Convolutional-Only Approach](https://arxiv.org/pdf/2312.00633.pdf)
* [PointBeV: A Sparse Approach to BeV Predictions](https://arxiv.org/pdf/2312.00703.pdf)
* [Simple-BEV: What Really Matters for Multi-Sensor BEV Perception?](https://arxiv.org/pdf/2206.07959.pdf)
* [WidthFormer: Toward Efficient Transformer-based BEV View Transformation](https://arxiv.org/pdf/2401.03836.pdf)


# BEV分割 (LaneGraph)
* [Learning and Aggregating Lane Graphs for Urban Automated Driving](https://arxiv.org/abs/2302.06175) 
* [InstaGraM Instance-level Graph Modeling for Vectorized HD Map Learning](https://arxiv.org/abs/2301.04470)
* [(2021) Structured Bird’s-Eye-View Traffic Scene Understanding from Onboard Images](https://arxiv.org/pdf/2110.01997.pdf)
* [(2022) Topology Preserving Local Road Network Estimation from Single Onboard Camera Image](https://openaccess.thecvf.com/content/CVPR2022/papers/Can_Topology_Preserving_Local_Road_Network_Estimation_From_Single_Onboard_Camera_CVPR_2022_paper.pdf)
* [(2022) Lane-Level Street Map Extraction from Aerial Imagery](https://openaccess.thecvf.com/content/WACV2022/papers/He_Lane-Level_Street_Map_Extraction_From_Aerial_Imagery_WACV_2022_paper.pdf)
* [(2023) Lane Graph as Path: Continuity-preserving Path-wise Modeling for Online Lane Graph Construction](https://github.com/hustvl/LaneGAP)
* [MapTR: Structured Modeling and Learning for Online Vectorized HD Map Construction](https://arxiv.org/pdf/2208.14437.pdf) [[Code]](https://github.com/hustvl/MapTR) [[笔记]](https://)
* [Neural Map Prior for Autonomous Driving](https://arxiv.org/pdf/2304.08481.pdf)
* [VMA: Divide-and-Conquer Vectorized Map Annotation System for Large-Scale Driving Scene](https://arxiv.org/pdf/2304.09807.pdf)
* [End-to-End Vectorized HD-map Construction with Piecewise Bezier Curve](https://openaccess.thecvf.com/content/CVPR2023/papers/Qiao_End-to-End_Vectorized_HD-Map_Construction_With_Piecewise_Bezier_Curve_CVPR_2023_paper.pdf)
* [MachMap: End-to-End Vectorized Solution for Compact HD-Map Construction](https://arxiv.org/pdf/2306.10301.pdf)
* [MV-Map: Offboard HD-Map Generation with Multi-view Consistency](https://arxiv.org/pdf/2305.08851.pdf)
* [LATR: 3D Lane Detection from Monocular Images with Transformer](https://arxiv.org/pdf/2308.04583.pdf)
* [MapTRv2: An End-to-End Framework for Online Vectorized HD Map Construction](https://arxiv.org/pdf/2308.05736v1.pdf)
* [InsightMapper: A Closer Look at Inner-instance Information for Vectorized High-Definition Mapping](https://arxiv.org/pdf/2308.08543.pdf)[[Code]]() [[笔记]](https://zhuanlan.zhihu.com/p/650732045)
* [StreamMapNet: Streaming Mapping Network for Vectorized Online HD Map Construction](https://arxiv.org/pdf/2308.12570.pdf)
* [PivotNet: Vectorized Pivot Learning for End-to-end HD Map Construction](https://arxiv.org/pdf/2308.16477.pdf)
* [MLP An Simple yet Strong Pipeline for Driving Topology Reasoning](https://arxiv.org/pdf/2310.06753.pdf)
* [ScalableMap: Scalable Map Learning for Online Long-Range Vectorized HD Map Construction](https://arxiv.org/pdf/2310.13378.pdf)
* [Augmenting Lane Perception and Topology Understanding with Standard Definition Navigation Maps](https://arxiv.org/pdf/2311.04079.pdf)
* [Graph-based Topology Reasoning for Driving Scenes](https://arxiv.org/pdf/2304.05277.pdf)
* [Separated RoadTopoFormer](https://arxiv.org/pdf/2307.01557.pdf)
* [Mind the map! Accounting for existing map information when estimating online HDMaps from sensor data](https://arxiv.org/pdf/2311.10517.pdf)
* [Online Vectorized HD Map Construction using Geometry](https://arxiv.org/pdf/2312.03341.pdf)

# BEV Occupancy
* [Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction](https://arxiv.org/pdf/2302.07817.pdf) [[Code]](https://github.com/wzzheng/TPVFormer) [[笔记]](https://zhuanlan.zhihu.com/p/648813002)
* [MonoScene Monocular 3D Semantic Scene Completion](https://arxiv.org/abs/2112.00726)
* [OccDepth: A Depth-aware Method for 3D Semantic Occupancy Network](https://arxiv.org/abs/2302.13540)
* [VoxFormer: Sparse Voxel Transformer for Camera-based 3D Semantic Scene Completion](https://arxiv.org/abs/2302.12251)
* [OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception](https://github.com/JeffWang987/OpenOccupancy)
* [A Simple Attempt for 3D Occupancy Estimation in Autonomous Driving](https://github.com/GANWANSHUI/SimpleOccupancy)
* [Occ-BEV: Multi-Camera Unified Pre-training via 3D Scene Reconstruction](https://arxiv.org/pdf/2305.18829.pdf)
* [Learning Occupancy for Monocular 3D Object Detection](https://arxiv.org/pdf/2305.15694.pdf)
* [Scene as Occupancy](https://arxiv.org/pdf/2306.02851.pdf) [[Code]](https://github.com/OpenDriveLab/OccNet)
* [UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering](https://arxiv.org/pdf/2306.09117.pdf)
* [FB-OCC 3D Occupancy Prediction based on Forward-Backward View Transformation](https://opendrivelab.com/e2ead/AD23Challenge/Track_3_NVOCC.pdf?=&linkId=100000205404832)[[Code]](https://github.com/NVlabs/FB-BEV) [【阅读笔记】](https://zhuanlan.zhihu.com/p/642730470)
* [OVO: Open-Vocabulary Occupancy](https://arxiv.org/pdf/2305.16133.pdf)
* [OCTraN: 3D Occupancy Convolutional Transformer Network in Unstructured Traffic Scenarios](https://arxiv.org/pdf/2307.10934.pdf)
* [RenderOcc: Vision-Centric 3D Occupancy Prediction with 2D Rendering Supervision](https://arxiv.org/pdf/2309.09502.pdf)
* [OccupancyDETR: Making Semantic Scene Completion as Straightforward as Object Detection](https://arxiv.org/pdf/2309.08504.pdf)
* [LiDAR-based 4D Occupancy Completion and Forecasting](https://arxiv.org/pdf/2310.11239.pdf)
* [SelfOcc: Self-Supervised Vision-Based 3D Occupancy Prediction](https://arxiv.org/pdf/2311.12754.pdf)
* [OccNeRF: Self-Supervised Multi-Camera Occupancy Prediction with Neural Radiance Fields](https://arxiv.org/pdf/2312.09243.pdf)


# 矢量化
* [PolyFormer Referring Image Segmentation as Sequential Polygon Generation](https://arxiv.org/abs/2302.07387)[[阅读笔记]](https://zhuanlan.zhihu.com/p/650991896)


# 多传感器融合
* [Transformers-Sensor-Fusion](https://github.com/ApoorvRoboticist/Transformers-Sensor-Fusion)
* [UniTR: A Unified and Efficient Multi-Modal Transformer for Bird’s-Eye-View Representation]()

# 检测跟踪一体化
* [Exploring Simple 3D Multi-Object Tracking for Autonomous Driving](https://github.com/qcraftai/simtrack)
* [MOTR: End-to-End Multiple-Object Tracking with TRansformer](https://github.com/megvii-research/MOTR)
* [MUTR3D: A Multi-camera Tracking Framework via 3D-to-2D Queries](https://tsinghua-mars-lab.github.io/mutr3d/)
* [MotionTrack: End-to-End Transformer-based Multi-Object Tracing with LiDAR-Camera Fusion](https://arxiv.org/pdf/2306.17000.pdf)
* [3DMOTFormer: Graph Transformer for Online 3D Multi-Object Tracking](https://arxiv.org/pdf/2308.06635.pdf)


# Prediction
* [Motion Transformer with Global Intention Localization and Local Movement Refinement](https://arxiv.org/pdf/2209.13508.pdf)

# Localization
* [U-BEV: Height-aware Bird's-Eye-View Segmentation and Neural Map-based Relocalization](https://arxiv.org/pdf/2310.13766.pdf)


# Calibration
* [☆] [Calib-Anything: Zero-training LiDAR-Camera Extrinsic Calibration Method Using Segment Anything](https://arxiv.org/pdf/2306.02656.pdf)

# Simulation 
* [UniSim: A Neural Closed-Loop Sensor Simulator](https://waabi.ai/wp-content/uploads/2023/05/UniSim-paper.pdf)
* [Rethinking Closed-loop Training for Autonomous Driving](https://arxiv.org/pdf/2306.15713.pdf)


# Transformer
* [SegViTv2: Exploring Efficient and Continual Semantic Segmentation with Plain Vision Transformers](https://arxiv.org/pdf/2306.06289.pdf)
* [Taming Transformers for High-Resolution Image Synthesis](https://arxiv.org/pdf/2012.09841.pdf)
* [Mask2Former: Masked-attention Mask Transformer for Universal Image Segmentation](https://arxiv.org/pdf/2112.01527.pdf) [[笔记]](https://zhuanlan.zhihu.com/p/644070743)

# Survey
* [Milestones in Autonomous Driving and Intelligent Vehicles Survey of Surveys](https://arxiv.org/pdf/2303.17220.pdf)
* [Milestones_in_autonomous_driving_for_intelligent_vehicles_Pt1-2023](https://arxiv.org/pdf/2305.11239.pdf)
* [Milestones in Autonomous Driving and Intelligent Vehicles Part II Perception and Planning](https://arxiv.org/pdf/2306.01980.pdf)
* [Transformer-based models and hardware acceleration analysis in autonomous driving A survey](https://arxiv.org/pdf/2304.10891.pdf)

# Self-supervised
* [Linking vision and motion for self-supervised object-centric perception](https://arxiv.org/pdf/2307.07147.pdf)

# Nerf
* [MapNeRF: Incorporating Map Priors into Neural Radiance Fields for Driving View Simulation](https://arxiv.org/pdf/2307.14981.pdf)[[阅读笔记]](https://zhuanlan.zhihu.com/p/653414617)
* [NeRF-Det: Learning Geometry-Aware Volumetric Representation for Multi-View 3D Object Detection](https://arxiv.org/pdf/2307.14620v1.pdf)
* [Grid-guided Neural Radiance Fields for Large Urban Scenes](https://arxiv.org/pdf/2303.14001.pdf) [[Code]](https://github.com/InternLandMark/LandMark) [[阅读笔记]](https://zhuanlan.zhihu.com/p/647868981)
* [Horizon Robotics: RoMe: Towards Large Scale Road Surface Reconstruction via Mesh Representation](https://arxiv.org/pdf/2306.11368.pdf)
* [Street-View Image Generation from a Bird’s-Eye View Layout](https://arxiv.org/pdf/2301.04634.pdf)
* [BEVControl: Accurately Controlling Street-view Elements with Multi-perspective Consistency via BEV Sketch Layout](https://arxiv.org/pdf/2308.01661.pdf)
* [A Vision-Centric Approach for Static Map Element Annotation](https://arxiv.org/pdf/2309.11754v1.pdf)

# Diffusion
* [License Plate Super-Resolution Using Diffusion Models](https://arxiv.org/pdf/2309.12506.pdf)
* [DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving](https://arxiv.org/pdf/2309.09777.pdf)
* [GAIA-1: A Generative World Model for Autonomous Driving](https://arxiv.org/pdf/2309.17080.pdf)
* [Learning Universal Policies via Text-Guided Video Generation](https://arxiv.org/pdf/2302.00111.pdf)
* [Scaling Robot Learning with Semantically Imagined Experience](https://arxiv.org/pdf/2302.11550.pdf)
* [Synthetic Experience Replay](https://openreview.net/pdf?id=0a9p3Ty2k_)
* [Diffusion Model-Augmented Behavioral Cloning](https://arxiv.org/pdf/2302.13335.pdf)
* [Planning with Diffusion for Flexible Behavior Synthesis](https://arxiv.org/pdf/2205.09991.pdf)
* [Is Conditional Generative Modeling all you need for Decision-Making?](https://arxiv.org/pdf/2211.15657.pdf)
* [Imitating Human Behaviour with Diffusion Models](https://arxiv.org/pdf/2301.10677.pdf)
* [Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning](https://arxiv.org/pdf/2208.06193.pdf)
* [Efficient Diffusion Policies for Offline Reinforcement Learning](https://arxiv.org/pdf/2305.20081.pdf)
* [Diffusion Policy: Visuomotor Policy Learning via Action Diffusion](https://arxiv.org/pdf/2303.04137.pdf)
* [Crossway Diffusion: Improving Diffusion-based Visuomotor Policy via Self-supervised Learning](https://arxiv.org/pdf/2307.01849.pdf)
* [A Survey on Video Diffusion Models](https://arxiv.org/pdf/2310.10647.pdf)
* [MagicDrive: Street View Generation with Diverse 3D Geometry Control](https://arxiv.org/pdf/2310.02601.pdf)
* [DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model](https://arxiv.org/pdf/2310.07771.pdf)
* [DrivingGaussian: Composite Gaussian Splatting for Surrounding Dynamic Autonomous Driving Scenes](https://arxiv.org/pdf/2312.07920.pdf)

# CV
* [Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation](https://arxiv.org/pdf/2206.02777.pdf)
* [Segment Everything Everywhere All at Once](https://arxiv.org/pdf/2304.06718.pdf)
* [Multi-modal Queried Object Detection in the Wild](https://arxiv.org/abs/2305.18980)
* [Traffic Sign Interpretation in Real Road Scene](https://arxiv.org/pdf/2311.10793.pdf)
* [Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection](https://arxiv.org/pdf/2303.05499.pdf)
* [EfficientSAM Leveraged Masked Image Pretraining for Efficient Segment Anything](https://arxiv.org/pdf/2312.00863.pdf)
* [General Object Foundation Model for Images and Videos at Scale](https://arxiv.org/pdf/2312.09158.pdf)
* [Detecting Everything in the Open World: Towards Universal Object Detection](https://arxiv.org/pdf/2303.11749.pdf)
* [SCLIP: Rethinking Self-Attention for Dense Vision-Language Inference](https://arxiv.org/pdf/2312.01597.pdf)
* [MobileSAMv2: Faster Segment Anything to Everything](https://arxiv.org/pdf/2312.09579.pdf)
* [An Empirical Model of Large-Batch Training](https://arxiv.org/pdf/1812.06162.pdf)
* [Open-Vocabulary SAM: Segment and Recognize Twenty-thousand Classes Interactively](https://arxiv.org/pdf/2401.02955.pdf)


