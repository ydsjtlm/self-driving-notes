# LLM
* [☆X][ChatGPT is not all you need. A State of the Art Review of large Generative AI models](https://arxiv.org/abs/2301.04655)
* [☆X][Multimodal Deep Learning](https://arxiv.org/pdf/2301.04856.pdf)
* [☆][Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)  [【阅读笔记】](https://zhuanlan.zhihu.com/p/641016232)
* [☆][Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)  [【阅读笔记】](https://zhuanlan.zhihu.com/p/634627008)
* [☆][BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/638895437)
* [☆][GPT1: Improving Language Understanding by Generative Pre-Training](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)  [【阅读笔记】](https://zhuanlan.zhihu.com/p/638298091)
* [☆][GPT2: Language Models are Unsupervised Multitask Learners](https://life-extension.github.io/2020/05/27/GPT%E6%8A%80%E6%9C%AF%E5%88%9D%E6%8E%A2/language-models.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/638298091)
* [☆][GPT3: Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)  [【阅读笔记】](https://zhuanlan.zhihu.com/p/638298091)
* [☆][InstructGPT: Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155.pdf)  [【阅读笔记】](https://zhuanlan.zhihu.com/p/638298091)
* [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension]()
* [Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556.pdf)
* [Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361.pdf)

* [☆][ViT: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/pdf/2010.11929.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/637849428)
* [☆][Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/pdf/2103.14030.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/637849428)
* [☆][DETR: End-to-End Object Detection with Transformers](https://arxiv.org/pdf/2005.12872.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/637849428)
* [☆][Deformable DETR Deformable Transformers for End-to-End Object Detection](https://arxiv.org/pdf/2010.04159.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/637849428)
* [Visual Programming: Compositional visual reasoning without training](https://arxiv.org/pdf/2211.11559.pdf)
* [Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models](https://arxiv.org/pdf/2306.08641.pdf)

# AIGC
* [☆] [VQGAN: Taming Transformers for High-Resolution Image Synthesis](https://arxiv.org/pdf/2012.09841.pdf)[【阅读笔记】](https://zhuanlan.zhihu.com/p/641471772)
* [☆] [DDPM: Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239.pdf)[【阅读笔记】](https://zhuanlan.zhihu.com/p/641471772)
* [☆] [DALL-E2: Hierarchical Text-Conditional Image Generation with CLIP Latents](https://arxiv.org/pdf/2204.06125.pdf)[【阅读笔记】](https://zhuanlan.zhihu.com/p/641471772)
* [☆] [DALL-E: Zero-Shot Text-to-Image Generation](https://arxiv.org/pdf/2102.12092.pdf)[【阅读笔记】](https://zhuanlan.zhihu.com/p/641471772)
* [☆] [Stable Diffusion: High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/pdf/2112.10752.pdf)[【阅读笔记】](https://zhuanlan.zhihu.com/p/641471772)
* [Imagen: Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding](https://arxiv.org/pdf/2205.11487.pdf)[【阅读笔记】](https://zhuanlan.zhihu.com/p/641471772)
* [☆] [RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths](https://arxiv.org/pdf/2305.18295.pdf)[【阅读笔记】](https://zhuanlan.zhihu.com/p/641471772)
* [☆] [CM3leon: Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning](https://scontent-lax3-1.xx.fbcdn.net/v/t39.2365-6/358725877_789390529544546_1176484804732743296_n.pdf?_nc_cat=108&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=_diQr9c6Ru8AX8AL77d&_nc_ht=scontent-lax3-1.xx&oh=00_AfD8JGolEO3QmSnrkpzQnbv533aSj2eHNm0at6cJ2epOpg&oe=64BB4972)
* [VideoControlNet: A Motion-Guided Video-to-Video Translation Framework by Using Diffusion Model with ControlNet](https://arxiv.org/pdf/2307.14073.pdf)
* [Control-A-Video: Controllable Text-to-Video Generation with Diffusion Models](https://arxiv.org/pdf/2305.13840.pdf)
* [VideoGPT: Video Generation using VQ-VAE and Transformers](https://arxiv.org/pdf/2104.10157.pdf)
* [Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets](https://arxiv.org/pdf/2311.15127)
* [Generative Image as Action Models](https://arxiv.org/pdf/2407.07875)
* [Lumina-T2X: Transforming Text into Any Modality, Resolution, and Duration via Flow-based Large Diffusion Transformers](https://arxiv.org/pdf/2405.05945)
* [DriveDiTFit: Fine-tuning Diffusion Transformers for Autonomous Driving](https://arxiv.org/pdf/2407.15661v1)
* [HoloDreamer: Holistic 3D Panoramic World Generation from Text Descriptions](https://arxiv.org/pdf/2407.15187)
* [CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer](https://arxiv.org/pdf/2408.06072)
* [Panacea: Panoramic and Controllable Video Generation for Autonomous Driving](https://arxiv.org/pdf/2311.16813)
* [Panacea+: Panoramic and Controllable Video Generation for Autonomous Driving](https://arxiv.org/pdf/2408.07605)
* [ControlNeXt: Powerful and Efficient Control for Image and Video Generation](https://arxiv.org/pdf/2408.06070)
* [xGen-VideoSyn-1: High-fidelity Text-to-Video Synthesis with Compressed Representations](https://arxiv.org/pdf/2408.12590v1)
* [Latte: Latent Diffusion Transformer for Video Generation](https://arxiv.org/pdf/2401.03048)
* [OmniGen: Unified Image Generation](https://arxiv.org/pdf/2409.11340v1)
* [DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation](https://arxiv.org/pdf/2410.13571v1)
* [FreeVS: Generative View Synthesis on Free Driving Trajectory](https://arxiv.org/pdf/2410.18079v1)
* [X-DRIVE: CROSS-MODALITY CONSISTENT MULTISENSOR DATA SYNTHESIS FOR DRIVING SCENARIOS](https://arxiv.org/pdf/2411.01123v1)
* [Randomized Autoregressive Visual Generation](https://arxiv.org/pdf/2411.00776)
* [HunyuanVideo: A Systematic Framework For Large Video Generative Models](https://github.com/Tencent/HunyuanVideo)
* [Infinity∞: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis](https://arxiv.org/pdf/2412.04431)
* [You See it, You Got it: Learning 3D Creation on Pose-Free Videos at Scale](https://arxiv.org/pdf/2412.06699)


# Multimodal (Vision/Language/Audio)
* [☆] [Segment Anything](https://arxiv.org/pdf/2304.02643.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/630808078)
* [☆] [Segment and Track Anything](https://arxiv.org/pdf/2305.06558.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/630987885)
* [☆] [Track Anything: Segment Anything Meets Videos](https://arxiv.org/pdf/2304.11968.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/630987885)
* [☆] [CLIP: Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020.pdf)[【阅读笔记】](https://zhuanlan.zhihu.com/p/631046116)
* [☆] [ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://arxiv.org/pdf/2102.03334.pdf)[【阅读笔记】](https://zhuanlan.zhihu.com/p/635802911)
* [☆] [ALBEF: Align before Fuse: Vision and Language Representation Learning with Momentum Distillation](https://arxiv.org/pdf/2107.07651.pdf)  [【阅读笔记】](https://zhuanlan.zhihu.com/p/635127422/edit)
* [☆] [VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts](https://arxiv.org/pdf/2111.02358.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/635116412)
* [☆] [Meta-Transformer: A Unified Framework for Multimodal Learning](https://arxiv.org/pdf/2307.10802.pdf) [[阅读笔记]](https://zhuanlan.zhihu.com/p/645727989)
* [☆] [ImageBind: One Embedding Space To Bind Them All](https://arxiv.org/pdf/2305.05665.pdf) [[阅读笔记]](https://zhuanlan.zhihu.com/p/645780641)
* [☆] [beitv3: Image as a Foreign Language BEiT Pretraining for All Vision and Vision-Language Tasks](https://arxiv.org/pdf/2208.10442.pdf) [[阅读笔记]](https://zhuanlan.zhihu.com/p/645983189)
* [☆] [PandaGPT: One Model To Instruction-Follow Them All](https://arxiv.org/pdf/2305.16355.pdf) [[阅读笔记]](https://zhuanlan.zhihu.com/p/645780641)
* [☆] [KOSMOS: Language is not all you Need Aligning Perception with Language Models](https://arxiv.org/pdf/2302.14045.pdf)[【阅读笔记】](https://zhuanlan.zhihu.com/p/631396292)
* [☆] [KOSMOS-2: Grounding Multimodal Large Language Models to the World](https://arxiv.org/pdf/2306.14824.pdf)[【阅读笔记】](https://zhuanlan.zhihu.com/p/631396292)
* [☆] [LLaVA: Visual Instruction Tuning](https://arxiv.org/pdf/2304.08485.pdf)[【阅读笔记】](https://zhuanlan.zhihu.com/p/636584622)
* [☆] [Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models](https://arxiv.org/pdf/2306.05424.pdf)[【阅读笔记】](https://zhuanlan.zhihu.com/p/636584622)
* [☆] [PG-Video-LLaVA: Pixel Grounding Large Video-Language Models](https://arxiv.org/pdf/2311.13435.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/636584622)
* [☆] [Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding](https://arxiv.org/pdf/2306.02858.pdf)[【阅读笔记】](https://zhuanlan.zhihu.com/p/636854782)  
* [☆] [MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models](https://arxiv.org/pdf/2304.10592.pdf)[【阅读笔记】](https://zhuanlan.zhihu.com/p/636854782)  
* [☆] [BLIP2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/pdf/2301.12597.pdf)[【阅读笔记】](https://zhuanlan.zhihu.com/p/636854782)    [[Code]](https://github.com/salesforce/LAVIS)
* [☆] [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://arxiv.org/pdf/2305.06500.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/636854782)   [[Code]](https://github.com/salesforce/LAVIS)
* [☆] [Flamingo: a Visual Language Model for Few-Shot Learning](https://arxiv.org/pdf/2204.14198.pdf) [[阅读笔记]](https://zhuanlan.zhihu.com/p/645750137)
* [☆] [PaLM-E An Embodied Multimodal Language Model](https://arxiv.org/pdf/2303.03378.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/637430937)
* [☆] [RT-2 Vision-Language-Action Models Transfer Web Knowledge to Robotic Control](https://robotics-transformer2.github.io/assets/rt2.pdf)[[阅读笔记]](https://zhuanlan.zhihu.com/p/646729912)
* [☆] [VisionLLM Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks](https://arxiv.org/pdf/2305.11175.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/639467790)
* [☆] [InternGPT: Solving Vision-Centric Tasks by Interacting with ChatGPT Beyond Language](https://arxiv.org/pdf/2305.05662.pdf)  [【阅读笔记】](https://zhuanlan.zhihu.com/p/639505432)
* [☆] [Segment Any Point Cloud Sequences by Distilling Vision Foundation Models](https://arxiv.org/pdf/2306.09347.pdf)  [【阅读笔记】](https://zhuanlan.zhihu.com/p/640760232)
* [☆] [综述 A Survey on Multimodal Large Language Models](https://arxiv.org/pdf/2306.13549.pdf)  [[Github]](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)  [【阅读笔记】](https://zhuanlan.zhihu.com/p/641261865)
* [☆] [综述 A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223.pdf) [[阅读笔记]](https://zhuanlan.zhihu.com/p/642003921)
* [☆] [评测：MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models](https://arxiv.org/pdf/2306.13394.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/641261865)
* [☆] [评测：C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models](https://arxiv.org/pdf/2305.08322.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/641261865)
* [☆X][Vision Language Transformers: A Survey](https://arxiv.org/pdf/2307.03254.pdf)
* [☆] [VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models](https://voxposer.github.io/voxposer.pdf)   [【阅读笔记】](https://zhuanlan.zhihu.com/p/644424405)
* [☆] [Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks](https://arxiv.org/pdf/2206.08916.pdf)   [[阅读笔记]](https://zhuanlan.zhihu.com/p/644096922)
* [☆] [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/pdf/2307.09288.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/645230336)
* [☆] [LLaMA Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971.pdf)[【阅读笔记】](https://zhuanlan.zhihu.com/p/642719013)
* [☆] [Scaling Instruction-Finetuned Language Models](https://arxiv.org/pdf/2210.11416.pdf)  [[阅读笔记]](https://zhuanlan.zhihu.com/p/645317908) 
* [☆] [LONGNET: Scaling Transformers to 1,000,000,000 Tokens](https://arxiv.org/pdf/2307.02486.pdf) [[阅读笔记]](https://zhuanlan.zhihu.com/p/645326943)
* [☆] [Faster Segment Anything: Towards Lightweight SAM for Mobile Applications](https://arxiv.org/pdf/2306.14289.pdf) [[阅读笔记]](https://zhuanlan.zhihu.com/p/645714778)  
* [☆] [Shikra: Unleashing Multimodal LLM’s Referential Dialogue Magic](https://arxiv.org/pdf/2306.15195.pdf) [[阅读笔记]](https://zhuanlan.zhihu.com/p/645787351)
* [☆] [Textbooks Are All You Need](https://arxiv.org/pdf/2306.11644.pdf)
* [☆] [3D-LLM: Injecting the 3D World into Large Language Models](https://arxiv.org/pdf/2307.12981.pdf) [[阅读笔记]](https://zhuanlan.zhihu.com/p/646624803)
* [☆] [LoRA Low-Rank Adaptation of Large Language Models](https://arxiv.org/pdf/2106.09685.pdf) [[阅读笔记]](https://zhuanlan.zhihu.com/p/646629579)
* [☆] [AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn](https://arxiv.org/pdf/2306.08640.pdf) [[阅读笔记]](https://zhuanlan.zhihu.com/p/646689611)
* [☆] [HuggingGPT Solving AI Tasks with ChatGPT and its Friends in HuggingFace](https://arxiv.org/pdf/2303.17580.pdf) [[阅读笔记]](https://zhuanlan.zhihu.com/p/646689611)
* [☆] [NExT-GPT: Any-to-Any Multimodal LLM](https://arxiv.org/pdf/2309.05519.pdf) [[阅读笔记]](https://zhuanlan.zhihu.com/p/661419123)
* [☆] [MINIGPT-5: INTERLEAVED VISION-AND-LANGUAGE GENERATION VIA GENERATIVE VOKENS](https://browse.arxiv.org/pdf/2310.02239v1.pdf) [[阅读笔记]](https://zhuanlan.zhihu.com/p/661419123)


* [CLIP分割：Language-driven Semantic Segmentation](https://arxiv.org/pdf/2201.03546.pdf)
* [CLIP分割：GroupViT: Semantic Segmentation Emerges from Text Supervision](https://arxiv.org/pdf/2202.11094.pdf)
* [CLIP检测：Open-vocabulary Object Detection via Vision and Language Knowledge Distillation](https://arxiv.org/pdf/2104.13921.pdf)
* [CLIP检测：Grounded Language-Image Pre-training](https://arxiv.org/pdf/2112.03857.pdf)
* [CLIPasso Semantically-Aware Object Sketching](https://arxiv.org/pdf/2202.05822.pdf)
* [Contextual Object Detection with Multimodal Large Language Models](https://arxiv.org/pdf/2305.18279.pdf) [[Code]](https://github.com/yuhangzang/ContextDET)

* [QLoRA Efficient Finetuning of Quantized LLMs](https://arxiv.org/pdf/2305.14314.pdf)


* [LIMA Less Is More for Alignment](https://arxiv.org/pdf/2305.11206.pdf)
* [FrugalGPT How to Use Large Language Models While Reducing Cost and Improving Performance](https://arxiv.org/pdf/2305.05176.pdf)
* [SELF-INSTRUCT Aligning Language Models with Self-Generated Instructions](https://arxiv.org/pdf/2212.10560.pdf)
* [Toolformer Language Models Can Teach Themselves to Use Tools](https://arxiv.org/pdf/2302.04761.pdf)
* [Tree of Thoughts Deliberate Problem Solving with Large Language Models](https://arxiv.org/pdf/2305.10601.pdf)
* [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/pdf/2201.11903.pdf)

* [Whisper: Robust Speech Recognition via Large-Scale Weak Supervision](https://arxiv.org/pdf/2212.04356.pdf)
* [LaMDA Language Models for Dialog Applications](https://arxiv.org/pdf/2201.08239.pdf)

* [BLIP Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/pdf/2201.12086.pdf)

* [CoCa Contrastive Captioners are Image-Text Foundation Models](https://arxiv.org/pdf/2205.01917.pdf)
* [Fine-Tuning Language Models from Human Preferences](https://arxiv.org/pdf/1909.08593.pdf)
* [Anthropic LLM: Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/pdf/2204.05862.pdf)
* [GPTs are GPTs An Early Look at the Labor Market Impact Potential of Large Language Models](https://arxiv.org/pdf/2303.10130.pdf)
* [sparks of artificial general intelligence](https://arxiv.org/pdf/2303.12712.pdf)


* [Transformer models: an introduction and catalog](https://arxiv.org/pdf/2302.07730.pdf)
* [A Survey on Long Text Modeling with Transformers](https://arxiv.org/pdf/2302.14502.pdf)
* [A Comprehensive Survey on Pretrained Foundation Models A History from BERT to ChatGPT](https://arxiv.org/pdf/2302.09419.pdf)
* [Large Language Models Can Self-Improve](https://arxiv.org/pdf/2210.11610.pdf)

* [MovieChat From Dense Token to Sparse Memory for Long Video Understanding](https://arxiv.org/pdf/2307.16449.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/636854782)
* [LLM-GROUNDED VIDEO DIFFUSION MODELS](https://arxiv.org/pdf/2309.17444.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/661443505)
* [Code Llama: Open Foundation Models for Code](https://arxiv.org/pdf/2308.12950.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/661685730)
* [Siamese Masked Autoencoders](https://siam-mae-video.github.io/resources/paper.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/661683294)
* [MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning](https://arxiv.org/pdf/2310.09478.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/661892549)
* [Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation](https://arxiv.org/pdf/2308.07931.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/667631161)
* [Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models](https://arxiv.org/pdf/2303.04803.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/667633652)
* [SegPrompt: Boosting Open-world Segmentation via Category-level Prompt Learning](https://arxiv.org/pdf/2308.06531.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/667636980)
* [UniDoc: A Universal Large Multimodal Model for Simultaneous Text Detection, Recognition, Spotting and Understanding](https://arxiv.org/pdf/2308.11592.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/667639160)
* [NavGPT: Beyond Segmentation: Road Network Generation with Multi-Modal LLMs](https://arxiv.org/pdf/2310.09755.pdf) [【阅读笔记】]()
* [LISA: Reasoning Segmentation via Large Language Model](https://arxiv.org/pdf/2308.00692.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/667646754)
* [Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V](https://arxiv.org/pdf/2310.11441.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/667651095)
* [UnifiedVisionGPT: Streamlining Vision-Oriented AI through Generalized Multimodal Framework](https://arxiv.org/pdf/2311.10125.pdf) [【阅读笔记】](NoMore) 
* [Towards Open-Ended Visual Recognition with Large Language Model](https://arxiv.org/pdf/2311.08400.pdf) [【阅读笔记】]()
* [ShareGPT4V: Improving Large Multi-Modal Models with Better Captions](https://arxiv.org/pdf/2311.12793.pdf) [【阅读笔记】]()
* [An Embodied Generalist Agent in 3D World](https://arxiv.org/pdf/2311.12871.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/668856211)
* [The Curse of Recursion: Training on Generated Data Makes Models Forget](https://arxiv.org/pdf/2305.17493.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/668869579)
* [Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks](https://arxiv.org/pdf/2311.06242.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/668875847)


* [An Early Evaluation of GPT-4V(ision)](https://arxiv.org/pdf/2310.16534.pdf)
* [Video Language Planning](https://arxiv.org/pdf/2310.10625.pdf)
* [Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding](https://arxiv.org/pdf/2311.08046.pdf)
* [Frozen Transformers in Language Models Are Effective Visual Encoder Layers](https://arxiv.org/pdf/2310.12973.pdf)
* [On Efficient Training of Large-Scale Deep Learning Models: A Literature Review](https://arxiv.org/pdf/2304.03589.pdf)
* [GPT4Video: A Unified Multimodal Large Language Model for lnstruction-Followed Understanding and Safety-Aware Generation](https://arxiv.org/pdf/2311.16511.pdf)
* [Sequential Modeling Enables Scalable Learning for Large Vision Models](https://arxiv.org/pdf/2312.00785.pdf)
* [LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models](https://arxiv.org/pdf/2312.02949.pdf)
* [Gemini: A Family of Highly Capable Multimodal Models](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf)
* [Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning](https://arxiv.org/pdf/2311.17842.pdf)
* [Chain of Code: Reasoning with a Language Model-Augmented Code Emulator](https://arxiv.org/pdf/2312.04474.pdf)
* [WEAK-TO-STRONG GENERALIZATION: ELICITING STRONG CAPABILITIES WITH WEAK SUPERVISION](https://cdn.openai.com/papers/weak-to-strong-generalization.pdf)
* [LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models](https://arxiv.org/pdf/2311.17043.pdf)
* [VideoPoet: A Large Language Model for Zero-Shot Video Generation](https://storage.googleapis.com/videopoet/paper.pdf)
* [PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU](https://ipads.se.sjtu.edu.cn/_media/publications/powerinfer-20231219.pdf)
* [A Simple Recipe for Contrastively Pre-training Video-First Encoders Beyond 16 Frames](https://arxiv.org/pdf/2312.07395.pdf)
* [Mixtral of Experts](https://arxiv.org/pdf/2401.04088.pdf)
* [MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World](https://arxiv.org/pdf/2401.08577.pdf)
* [CognitiveDog: Large Multimodal Model Based System to Translate Vision and Language into Action of Quadruped Robot](https://arxiv.org/pdf/2401.09388.pdf)
* [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/ftp/arxiv/papers/2312/2312.00752.pdf)
* [LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers](https://arxiv.org/pdf/2310.03294.pdf)
* [LLM in a flash: Efficient Large Language Model Inference with Limited Memory](https://arxiv.org/pdf/2312.11514.pdf)
* [AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling](https://arxiv.org/pdf/2402.12226.pdf)
* [Model Composition for Multimodal Large Language Models](https://arxiv.org/pdf/2402.12750.pdf)
* [Lumiere: A Space-Time Diffusion Model for Video Generation](https://arxiv.org/pdf/2401.12945.pdf)
* [EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought](https://arxiv.org/pdf/2305.15021.pdf)
* [Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models](https://arxiv.org/pdf/2402.17177.pdf)
* [DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models](https://arxiv.org/pdf/2403.00818.pdf)
* [RT-H: Action Hierarchies Using Language](https://arxiv.org/pdf/2403.01823.pdf)
* [VideoMamba: State Space Model for Efficient Video Understanding](https://arxiv.org/pdf/2403.06977.pdf)
* [GiT: Towards Generalist Vision Transformer through Universal Language Interface](https://arxiv.org/pdf/2403.09394.pdf)
* [Mora: Enabling Generalist Video Generation via A Multi-Agent Framework](https://arxiv.org/pdf/2403.13248.pdf)
* [CLIP-BEVFormer: Enhancing Multi-View Image-Based BEV Detector with Ground Truth Flow](https://arxiv.org/pdf/2403.08919.pdf)
* [MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens](https://arxiv.org/pdf/2404.03413.pdf)
* [VAR_Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction](https://arxiv.org/pdf/2404.02905.pdf)
* [Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models](https://arxiv.org/pdf/2403.18814.pdf)
* [Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation](https://arxiv.org/pdf/2406.06525)
* [OpenVLA: An Open-Source Vision-Language-Action Model](https://arxiv.org/pdf/2406.09246)
* [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/pdf/2205.14135)
* [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://tridao.me/publications/flash2/flash2.pdf)
* [FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision](https://tridao.me/publications/flash3/flash3.pdf)
* [Unveiling Encoder-Free Vision-Language Models](https://arxiv.org/pdf/2406.11832)
* [DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of MLLM](https://arxiv.org/pdf/2403.12488)
* [LUMINA-MGPT: ILLUMINATE FLEXIBLE PHOTOREALISTIC TEXT-TO-IMAGE GENERATION WITH MULTIMODAL GENERATIVE PRETRAINING](https://arxiv.org/pdf/2408.02657v1)
* [DRIVEARENA: A Closed-loop Generative Simulation Platform for Autonomous Driving](https://arxiv.org/pdf/2408.00415v1)
* [The Llama 3 Herd of Models](https://arxiv.org/pdf/2407.21783v1)
* [VLM][Autoregressive Image Generation without Vector Quantization](https://arxiv.org/pdf/2406.11838)
* [VLM][Chameleon: Mixed-Modal Early-Fusion Foundation Models](https://arxiv.org/pdf/2405.09818)
* [VLM][Language Model Beats Diffusion Tokenizer is key to visual generation](https://arxiv.org/pdf/2310.05737v2)
* [DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search](https://www.arxiv.org/pdf/2408.08152)
* [VLM][Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model](https://arxiv.org/pdf/2408.11039v1)
* [VLM][MaVEn: An Effective Multi-granularity Hybrid Visual Encoding Framework for Multimodal Large Language Model](https://arxiv.org/pdf/2408.12321v1)
* [VLM][xGen-MM (BLIP-3): A Family of Open Large Multimodal Models](https://www.arxiv.org/pdf/2408.08872)
* [VLM][Building and better understanding vision-language models: insights and future directions](https://arxiv.org/pdf/2408.12637v1)
* [Large Language Monkeys: Scaling Inference Compute with Repeated Sampling](https://arxiv.org/pdf/2407.21787)
* [Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters](https://arxiv.org/pdf/2408.03314)
* [VLM][NVLM: Open Frontier-Class Multimodal LLMs](https://arxiv.org/pdf/2409.11402v1)
* [Qwen2-VL: Enhancing Vision-Language Model’s Perception of the World at Any Resolution](https://arxiv.org/pdf/2409.12191v1)
* [Emu3: Next-Token Prediction is All You Need](https://arxiv.org/pdf/2409.18869)
* [Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation](https://arxiv.org/pdf/2410.13848)
* [xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video Even in VLMs](https://arxiv.org/pdf/2410.16267v1)
* [☆][Q-VLM: Post-training Quantization for Large Vision-Language Models](https://arxiv.org/pdf/2410.08119v1)[VLM部署/VLM提速]
* [☆][SparseLLM: Towards Global Pruning of Pre-trained Language Models](https://arxiv.org/abs/2402.17946)[LLM部署/剪枝]
* [☆][Loong: Generating Minute-level Long Videos with Autoregressive Language Models](https://arxiv.org/pdf/2410.02757v1)
* [☆][LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding](https://arxiv.org/pdf/2410.17434v1)
* [☆][OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models](https://arxiv.org/pdf/2411.04905)
* [☆][VLA][CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation](https://arxiv.org/pdf/2411.19650v1)
* [☆][VLA][CombatVLA: An Efficient Vision-Language-Action Model for Combat Tasks in 3D Action Role-Playing Games](https://arxiv.org/pdf/2503.09527)
* [☆][VLA][HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model](https://arxiv.org/pdf/2503.10631)
* [☆][VLA][ChatVLA: Unified Multimodal Understanding and Robot Control with Vision-Language-Action Model](https://arxiv.org/pdf/2502.14420)
* [☆][VLA][Diffusion-VLA: Scaling Robot Foundation Models via Unified Diffusion and Autoregression](https://arxiv.org/pdf/2412.03293)[code](https://github.com/juruobenruo/DexVLA)
* [☆][VLA][DexVLA: Vision-Language Model with Plug-In Diffusion Expert for General Robot Control](https://arxiv.org/pdf/2502.05855)
* [☆☆][VLA][Scaling Diffusion Policy in Transformer to 1 Billion Parameters for Robotic Manipulation](https://arxiv.org/pdf/2409.14411)
* [☆][VLA][OpenDriveVLA: Towards End-to-end Autonomous Driving with Large Vision Language Action Model](https://arxiv.org/pdf/2503.23463)
* [☆][VLA][OpenVLA-OFT Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success](https://arxiv.org/pdf/2502.19645)[Parallel Decoding]
* [☆][VLA][Accelerating Vision-Language-Action Model Integrated with Action Chunking via Parallel Decoding](https://arxiv.org/pdf/2503.02310)[Parallel Decoding]
* [☆][VLA][VOTE: Vision-Language-Action Optimization with Trajectory Ensemble Voting](https://arxiv.org/pdf/2507.05116)[VLA/简化版cogact]
* [☆][Multimodal Latent Language Modeling with Next-Token Diffusion](https://arxiv.org/pdf/2412.08635)
* [☆][DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding](https://arxiv.org/pdf/2412.10302v1)
* [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/pdf/2402.03300)
* [☆][Byte Latent Transformer: Patches Scale Better Than Tokens](https://arxiv.org/pdf/2412.09871)
* [☆][FastVLM: Efficient Vision Encoding for Vision Language Models](https://arxiv.org/pdf/2412.13303)[VLM部署/VLM提速]
* [☆][LLM Post-Training: A Deep Dive into Reasoning Large Language Models](https://arxiv.org/pdf/2502.21321)[post-training/综述]
* [☆][Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/ORZ_paper.pdf)
* [☆][Visual-RFT: Visual Reinforcement Fine-Tuning](https://arxiv.org/pdf/2503.01785)
* [☆][Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models](https://arxiv.org/pdf/2503.06749)
* [☆][MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale Reinforcement Learning](https://arxiv.org/pdf/2503.07365)
* [☆][Predictable Scale: Part I — Optimal Hyperparameter Scaling Law in Large Language Model Pretraining](https://arxiv.org/pdf/2503.04715)
* [☆][Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models](https://arxiv.org/pdf/2503.09573)
* [☆][Understanding R1-Zero-Like Training: A Critical Perspective](https://github.com/sail-sg/understand-r1-zero/blob/main/understand-r1-zero.pdf)
* [☆][Video-T1: Test-Time Scaling for Video Generation](https://arxiv.org/pdf/2503.18942)
* [☆][EfficientLLaVA:Generalizable Auto-Pruning for Large Vision-language Models](https://arxiv.org/pdf/2503.15369) [VLM部署/VLM提速/剪枝]
* [☆][SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language Models for Long-Form Video Understanding](https://arxiv.org/pdf/2503.18943) [长视频理解/]
* [Unified Autoregressive Visual Generation and Understanding with Continuous Tokens](https://arxiv.org/pdf/2503.13436)
* [DAPO: An Open-Source LLM Reinforcement Learning System at Scale](https://dapo-sia.github.io/static/pdf/dapo_paper.pdf)
* [Qwen2.5-Omni Technical Report](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/assets/Qwen2.5_Omni.pdf)
* [☆][Video-R1: Reinforcing Video Reasoning in MLLMs](https://arxiv.org/pdf/2503.21776)
* [☆][MAGI-1: Autoregressive Video Generation at Scale](https://static.magi.world/static/files/MAGI_1.pdf)[TBD/视频生成]
* [☆][Bridging Continuous and Discrete Tokens for Autoregressive Visual Generation](https://arxiv.org/pdf/2503.16430)[后训练量化自回归生成token离散化问题]
* [☆][Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs](https://arxiv.org/pdf/2503.01307)[基座模型边界锁死RL能力上限]
* [☆][Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?](https://arxiv.org/pdf/2504.13837)[基座模型边界锁死RL能力上限]
* [☆][Rethinking Reflection in Pre-Training](https://arxiv.org/pdf/2504.04022)[基座模型边界锁死RL能力上限]
* [☆][TTRL: Test-Time Reinforcement Learning](https://arxiv.org/pdf/2504.16084)[基座模型边界锁死RL能力上限]
* [☆][Inference-Time Scaling for Generalist Reward Modeling](https://arxiv.org/pdf/2504.02495)[基座模型边界锁死RL能力上限]
* [☆][SimpleAR: Pushing the Frontier of Autoregressive Visual Generation through Pretraining, SFT, and RL](http://arxiv.org/pdf/2504.11455)[AR图像生成/偏工程性/架构简洁/训练优化/推理优化]
* [☆][Describe Anything: Detailed Localized Image and Video Captioning](https://arxiv.org/pdf/2504.16072)[图片/视频局部信息描述]
* [☆][SmolVLM: Redefining small and efficient multimodal models](https://arxiv.org/pdf/2504.05299)[模型小型化/VLM提速]
* [☆][Fast-Slow Thinking for Large Vision-Language Model Reasoning](https://arxiv.org/pdf/2504.18458)[VLM过度思考]
* [☆][NORA: A Small Open-Sourced Generalist Vision Language Action Model for Embodied Tasks](https://arxiv.org/pdf/2504.19854)[VLA+FAST-encoder]
* [☆][TinyLLaVA: A Framework of Small-scale Large Multimodal Models](https://arxiv.org/pdf/2402.14289)
* [☆][StreamBridge: Turning Your Offline Video Large Language Model into a Proactive Streaming Assistant](https://arxiv.org/pdf/2505.05467)[流式视频理解]
* [☆][LTX-Video: Realtime Video Latent Diffusion](https://arxiv.org/pdf/2501.00103)[视频生成、VAE/Diffusion]
* [☆][VLM][Seed1.5-VL Technical Report](https://arxiv.org/pdf/2505.07062)
* [☆][Flow-GRPO: Training Flow Matching Models via Online RL](https://www.arxiv.org/pdf/2505.05470)[图像视频生成+GRPO]
* [☆][Speculative Decoding Reimagined for Multimodal Large Language Models](https://arxiv.org/pdf/2505.14260)[VLM部署/VLM提速/多模态投机采样]
* [☆][UniGen: Enhanced Training & Test-Time Strategies for Unified Multimodal Understanding and Generation](https://arxiv.org/pdf/2505.14682)[TBD/生成&理解]
* [☆][Vid2World: Crafting Video Diffusion Models toInteractive World Models](https://arxiv.org/pdf/2505.14357)[TBD]
* [☆][MMaDA: Multimodal Large Diffusion Language Models](https://arxiv.org/pdf/2505.15809)[TBD]
* [☆][G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language Model via Reinforcement Learning](https://arxiv.org/pdf/2505.13426)[RL/VLM/游戏]
* [☆][VIDEORFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning](https://arxiv.org/pdf/2505.12434)[video+GRPO/dataset]
* [☆][LLaVA-4D: Embedding SpatioTemporal Prompt into LMMs for 4D Scene Understanding](https://arxiv.org/pdf/2505.12253)[VLM时空理解依赖depth]
* [☆][Interactive Post-Training for Vision-Language-Action Models](https://arxiv.org/pdf/2505.17016)[VLA+GRPO/RIPT-VLA]
* [☆][Multi-SpatialMLLM: Multi-Frame Spatial Understanding with MultiModal Large Language Models](https://arxiv.org/pdf/2505.17015)
* [☆][LaViDa: A Large Diffusion Language Model for Multimodal Understanding](https://arxiv.org/pdf/2505.16839)
* [☆][R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO](https://arxiv.org/pdf/2505.16673)
* [☆][Dimple: Discrete Diffusion Multimodal Large Language Model with Parallel Decoding](https://arxiv.org/pdf/2505.16990)
* [☆][Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models](https://arxiv.org/pdf/2505.10446)
* [☆][TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming Videos](https://arxiv.org/pdf/2504.17343)[长视频理解/]
* [☆][QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning](https://www.arxiv.org/pdf/2505.17667)[外挂文本的推理+RL]
* [☆][GeoDrive: 3D Geometry-Informed Driving World Model with Precise Action Control](https://arxiv.org/pdf/2505.22421)[自动驾驶worldmodel]
* [☆][3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Mode](https://arxiv.org/pdf/2505.22421)[VLM时空理解]
* [☆][Spurious Rewards: Rethinking Training Signals in RLVR](https://github.com/ruixin31/Rethink_RLVR/blob/main/paper/rethink-rlvr.pdf)
* [☆][Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding](https://arxiv.org/pdf/2505.22618)[TBD]
* [☆][SFT or RL? An Early Investigation into Training R1-Like Reasoning Large Vision-Language Models](https://arxiv.org/pdf/2504.11468)[MVLM任务SFT会影响RL性能]
* [☆][VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model](https://arxiv.org/pdf/2504.07615)
* [☆][DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models](https://arxiv.org/pdf/2505.24025)[DINO+GRPO提升检测精度]
* [☆][UniWorld: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation](https://arxiv.org/pdf/2506.03147)[生成&理解]
* [☆][ReasonPlan: Unified Scene Prediction and Decision Reasoning for Closed-loop Autonomous Driving](https://arxiv.org/pdf/2505.20024)
* [Reinforcement Learning Enhanced LLMs: A Survey](https://simg.baai.ac.cn/paperfile/027ecaff-f505-436f-bbb0-1fc0dfbfba54.pdf)
* [☆][DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware Regressive GRPO](https://arxiv.org/pdf/2506.07464)[GRPO改进/视频理解]
* [☆][Magistral RL](https://mistral.ai/static/research/magistral.pdf)
* [☆][EfficientVLA: Training-Free Acceleration and Compression for Vision-Language-Action Models](https://arxiv.org/pdf/2506.10100)[VLA加速]
* [☆][PEVLM: Parallel Encoding for Vision-Language Models](https://arxiv.org/pdf/2506.19651)[VLM/VLA/部署/加速/Parallel Encoding]
* [☆][ParallelComp: Parallel Long-Context Compressor for Length Extrapolation](https://arxiv.org/pdf/2502.14317)[Parallel Encoding]
* [☆][APE: Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding](https://arxiv.org/pdf/2502.05431)[Parallel Encoding]
* [☆][Star Attention: Efficient LLM Inference over Long Sequences](https://arxiv.org/pdf/2411.17116)[Parallel Encoding]
* [☆][Block-attention for efficient prefilling](https://arxiv.org/abs/2409.15355)[Parallel Encoding]
* [☆][Welcome to the Era of Experience](https://storage.googleapis.com/deepmind-media/Era-of-Experience%20/The%20Era%20of%20Experience%20Paper.pdf)[经验时代]
* [☆][Cognition_Engineering](https://github.com/GAIR-NLP/cognition-engineering/blob/main/assets/Cognition_Engineering_zh.pdf)[认知工程]
* [☆][VLA/E2E][AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning](https://arxiv.org/pdf/2506.13757)[VLA/E2E/GRPO/RL]
* [☆][Real-Time Execution of Action Chunking Flow Policies](https://www.physicalintelligence.company/download/real_time_chunking.pdf)[VLA/Action Chunking/Physical Intelligence]
* [☆][Play to Generalize: Learning to Reason Through Game Play](https://arxiv.org/pdf/2506.08011)[GRPO游戏]
* [☆][Think When You Need: Self-Adaptive Chain-of-Thought Learning](https://arxiv.org/pdf/2504.03234)[GRPO/Think优化]

* [☆][HybridFlow: A Flexible and Efficient RLHF Framework](https://arxiv.org/pdf/2409.19256v2)
* [☆][LiteVLM: A Low-Latency Vision-Language Model Inference Pipeline for Resource-Constrained Environments](https://arxiv.org/pdf/2506.07416)[pass]
* [☆][4D-VLA: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration](https://arxiv.org/pdf/2506.22242)[对自动驾驶借鉴意义不大]
* [☆☆☆][A Survey on Vision-Language-Action Models for Autonomous Driving](https://arxiv.org/pdf/2506.24044)[Survey/VLA+AD+E2E非常全面]
* [☆][Epona: Autoregressive Diffusion World Model for Autonomous Driving](https://arxiv.org/pdf/2506.24113)[world model + E2E]
* [☆][World4Drive: End-to-End Autonomous Driving via Intention-aware Physical Latent World Model](https://arxiv.org/pdf/2507.00603)[world model + E2E/ TBD]
* [☆][WorldVLA: Towards Autoregressive Action World Model](https://arxiv.org/pdf/2506.21539)[world model + E2E]
* [☆][Unified Vision-Language-Action Model](https://arxiv.org/pdf/2506.19850)[VLA结合world model]
* [☆][DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge](https://arxiv.org/pdf/2507.04447)[简化版world model + VLA]
* [☆][VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers](https://arxiv.org/pdf/2507.01016)[action/VQVAE]
* [☆][MemOS: A Memory OS for AI System](https://statics.memtensor.com.cn/files/MemOS_0707.pdf)[LLM长期记忆]
* [☆][MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent](https://arxiv.org/pdf/2507.02259)[LLM/long-context]
* [☆][Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning](https://arxiv.org/pdf/2507.00432)[pass]
* [☆][MUON IS SCALABLE FOR LLM TRAINING](https://arxiv.org/pdf/2502.16982)
* [EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos](https://arxiv.org/pdf/2507.12440)
* [AutoVDC: Automated Vision Data Cleaning Using Vision-Language Models](https://www.arxiv.org/pdf/2507.12414)
* [VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning](https://arxiv.org/pdf/2507.13348)
* [☆][Streaming 4D Visual Geometry Transformer](https://arxiv.org/pdf/2507.11539)
* [☆][High-Resolution Visual Reasoning via Multi-Turn Grounding-Based Reinforcement Learning](https://arxiv.org/pdf/2507.05920)
* [KIMI K2: OPEN AGENTIC INTELLIGENCE](https://github.com/MoonshotAI/Kimi-K2/blob/main/tech_report.pdf)
* [☆][Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos](https://arxiv.org/pdf/2507.15597)[VLA/pre-train/VQVAE]
* [ReCamMaster: Camera-Controlled Generative Rendering from A Single Video](https://arxiv.org/pdf/2503.11647)
* [Your LLM Knows the Future Uncovering Its Multi-Token Prediction Potential](https://papers-pdfs.assets.alphaxiv.org/2507.11851v1.pdf)
* [YUME: AN INTERACTIVE WORLD GENERATION MODEL](https://arxiv.org/pdf/2507.17744)
* [Vision-Language-Action Instruction Tuning: From Understanding to Manipulation](https://arxiv.org/pdf/2507.17520)
* [PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving](https://arxiv.org/pdf/2507.17596)
* [A Survey on Vision-Language-Action Models: An Action Tokenization Perspective](https://www.alphaxiv.org/abs/2507.01925v1)
* [☆][Group Sequence Policy Optimization](https://arxiv.org/pdf/2507.18071v1)[GSPO]
* [☆][DriveAgent-R1: Advancing VLM-based Autonomous Driving with Hybrid Thinking and Active Perception](https://arxiv.org/pdf/2507.20879)[VLA/COT]
* [Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention](https://arxiv.org/pdf/2502.11089)
* [BLIP3-o: A Family of Fully Open Unified Multimodal Models—Architecture, Training and Dataset](https://www.arxiv.org/pdf/2505.09568)[统一理解+生成]
* [On-Device Diffusion Transformer Policy for Efficient Robot Manipulation](https://arxiv.org/pdf/2508.00697)[VLA/Diffusion加速]
* [FastDriveVLA: Efficient End-to-End Driving via Plug-and-Play Reconstruction-based Token Pruning](https://arxiv.org/pdf/2507.23318)[VLA+加速]
* [☆][VL-Cogito: Progressive Curriculum Reinforcement Learning for Advanced Multimodal Reasoning](https://arxiv.org/pdf/2507.22607)[VLM+RL]
* [☆][Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning](https://arxiv.org/pdf/2508.08221)[LLM中RL集大成]
* [☆][Reinforcement Learning in Vision: A Survey](https://www.alphaxiv.org/abs/2508.08189)[VLM+RL图片不错]
* [☆][GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models](https://www.alphaxiv.org/abs/2508.06471)
* [☆][On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification](https://www.alphaxiv.org/overview/2508.05629v1)
* [☆][R-Zero: Self-Evolving Reasoning LLM from Zero Data](https://papers-pdfs.assets.alphaxiv.org/2508.05004v1.pdf)[LLM、GAN？]
* [ReconDreamer-RL: Enhancing Reinforcement Learning via Diffusion-based Scene Reconstruction](https://arxiv.org/pdf/2508.08170)[仿真+RL+E2E]
* [☆☆][IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model](https://www.arxiv.org/pdf/2508.06571)[RL+VLA+E2E]
* [☆☆☆][RAD: Training an End-to-End Driving Policy via Large-Scale 3DGS-based Reinforcement Learning](https://arxiv.org/pdf/2502.13144)[E2E+3DGS+RL]
* [☆][VLM-3D: End-to-End Vision-Language Models for Open-World 3D Perception](https://arxiv.org/pdf/2508.09061)[VLM+3D]
* [A Review of 3D Object Detection with Vision-Language Models](https://arxiv.org/pdf/2504.18738)[VLM+3D/survey]
* [☆][DINOv3](https://arxiv.org/pdf/2508.10104)
* [Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models](https://arxiv.org/pdf/2508.10751)
* [☆][NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale](https://arxiv.org/pdf/2508.10711)[多模态统一架构+图片生成]
* [☆][SpaRC-AD: A Baseline for Radar-Camera Fusion in End-to-End Autonomous Driving](https://arxiv.org/pdf/2508.10567)[E2E+Sparse+多模态融合]
* [☆][FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving](https://arxiv.org/pdf/2505.17685)[E2E/world model]
* [Look-Back: Implicit Visual Re-focusing in MLLM Reasoning](https://arxiv.org/pdf/2507.03019)
* [☆☆☆][SimLingo: Vision-Only Closed-Loop Autonomous Driving with Language-Action Alignment](https://arxiv.org/pdf/2503.09594)[VLA+E2E+carla]
* [Prune2Drive: A Plug-and-Play Framework for Accelerating Vision-Language Models in Autonomous Driving](https://arxiv.org/pdf/2508.13305)[VLM/VLA+加速/E2E]
* [☆☆][DriveGPT4-V2: Harnessing Large Language Model Capabilities for Enhanced Closed-Loop Autonomous Driving](https://i.cs.hku.hk/~kykwong/publications/zxu_cvpr2025.pdf)[VLA+E2E]
* [☆][面向具身操作的视觉-语言-动作模型综述](https://arxiv.org/pdf/2508.15201)[survey]
* [☆☆][InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency](https://arxiv.org/pdf/2508.18265)
* [☆][AutoDrive-R²: Incentivizing Reasoning and Self-Reflection Capacity for VLA Model in Autonomous Driving](https://arxiv.org/pdf/2509.01944)[E2E/VAL/COT/GRPO+Reward/sft数据集]
* [☆][OmniReason: A Temporal-Guided Vision-Language-Action Framework for Autonomous Driving](https://arxiv.org/pdf/2509.00789)[E2E/VLA/COT/理想/sft数据集]
* [Speed Always Wins: A Survey on Efficient Architectures for Large Language Models](https://arxiv.org/pdf/2508.09834)[LLM架构survey]
* [Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory](https://arxiv.org/pdf/2508.09736v1)[agent+memory]
* [MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation](https://arxiv.org/pdf/2508.19236)[VLA+memory]
* [Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search](https://arxiv.org/pdf/2508.15884)
* [TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models](https://arxiv.org/pdf/2508.19257)[VLA+时序]
* [Perception in Plan: Coupled Perception and Planning for End-to-End Autonomous Driving](https://arxiv.org/pdf/2508.11488)[E2E]
* [Galaxea Open-World Dataset and G0 Dual-System VLA Model](https://arxiv.org/pdf/2509.00576)[VLA+双系统]

* [☆☆][DCPO: DYNAMIC CLIPPING POLICY OPTIMIZATION](https://arxiv.org/pdf/2509.02333)[GRPO/DCPO/RL]
* [LLaDA-VLA: Vision Language Diffusion Action Models](https://arxiv.org/pdf/2509.06932)[diffusion+VLA]
* [A VISION-LANGUAGE-ACTION MODEL BRIDGING UNDERSTANDING AND GENERATION TO ACTIONS](https://arxiv.org/pdf/2509.06951)[VLA+understanding+generation]
* [RL's Razor: Why Online Reinforcement Learning Forgets Less](https://www.arxiv.org/pdf/2509.04259v1)
* [Why Language Models Hallucinate](https://www.arxiv.org/pdf/2509.04664)[LLM幻觉]
* [Defeating Nondeterminism in LLM Inference](https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/)[LLM幻觉]
* [AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving](https://arxiv.org/pdf/2505.15298)[VLA/E2E/Agent]
* [The Landscape of Agentic Reinforcement Learning for LLMs: A Survey](https://arxiv.org/pdf/2509.02547)[LLM/Agent+RL]
* [SpecVLM: Fast Speculative Decoding in Vision-Language Models](https://arxiv.org/pdf/2509.11815)[VLM+加速+投机采样]
* [Perception Before Reasoning: Two-Stage Reinforcement Learning for Visual Reasoning in Vision-Language Models](https://arxiv.org/pdf/2509.13031)
* [3D Aware Region Prompted Vision Language Model](https://arxiv.org/pdf/2509.13317)
* [The Better You Learn, The Smarter You Prune: Towards Efficient Vision-language-action Models via Differentiable T](https://arxiv.org/pdf/2509.12594)[VLA+加速]

* [π0: A Vision-Language-Action Flow Model for General Robot Control](https://arxiv.org/pdf/2410.24164v1)[Physical Intelligence/AGI/VLA]
* [π0-FAST: Efficient Action Tokenization for Vision-Language-Action Models](https://arxiv.org/pdf/2501.09747)[Physical Intelligence/AGI/VLA]
* [π0.5: a Vision-Language-Action Model with Open-World Generalization](https://arxiv.org/pdf/2504.16054)[Physical Intelligence/AGI/VLA]
* [Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better](https://arxiv.org/pdf/2505.23705)[Physical Intelligence/AGI/VLA]

* [WALL-OSS: Igniting VLMs toward the Embodied Space](https://x2robot.cn-wlcb.ufileos.com/wall_oss.pdf)[AGI/VLA]
* [Reinforcement Learning Foundations for Deep Research Systems: A Survey](https://arxiv.org/pdf/2509.06733)[Deep Research]
* [Deep Research: A Survey of Autonomous Research Agents](https://arxiv.org/abs/2508.12752)[Deep Research]
* [☆][A Survey of Reinforcement Learning for Large Reasoning Models](https://arxiv.org/pdf/2509.08827)[Survey/RL/Reasoning见总结文档]
* [☆][SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning](https://arxiv.org/pdf/2509.09674)[VLA+RL]
* [☆][CoReVLA: A Dual-Stage End-to-End Autonomous Driving Framework for Long-Tail Scenarios via Collect-and-Refine](https://arxiv.org/abs/2509.15968v1)[VLA+E2E/DPO/SFT数据集]
* [☆][DriveDPO: Policy Learning via Safety DPO For End-to-End Autonomous Driving](http://arxiv.org/pdf/2509.17940v1)[E2E/RL]
* [☆☆][MTRDrive: Memory-Tool Synergistic Reasoning for Robust Autonomous Driving in Corner Cases](https://arxiv.org/abs/2509.20843)[VLA/E2E/COT/memory]
* [☆][DeepSeek_V3_2](https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf)
* [☆][MiniCPM_V_4_5_Technical_Report](https://arxiv.org/pdf/2509.18154)[MiniCPM/多模态基座]
* [☆][Qwen3-Omni Technical Report](https://papers-pdfs.assets.alphaxiv.org/2509.17765v1.pdf)
* [☆][Video models are zero-shot learners and reasoners](https://papers-pdfs.assets.alphaxiv.org/2509.20328v1.pdf)
* [☆][LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training](https://www.arxiv.org/pdf/2509.23661)[VLM数据集]
* [☆][Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers](https://arxiv.org/pdf/2510.11370)[LLM/MOE+RL]
* [SLAM-Former: Putting SLAM into One Transformer](https://arxiv.org/abs/2509.16909)[SLAM+LLM]
* [Pure Vision Language Action (VLA) Models: A Comprehensive Survey](https://arxiv.org/abs/2509.19012)[VLA/Survey]
* [NavDP: Learning Sim-to-Real Navigation Diffusion Policy with Privileged Information Guidance](https://arxiv.org/pdf/2505.08712)
* [Reinforcement Learning on Pre-Training Data](https://papers-pdfs.assets.alphaxiv.org/2509.19249v2.pdf)
* [Embodied AI: From LLMs to World Models](https://papers-pdfs.assets.alphaxiv.org/2509.20021v1.pdf)
* [Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation](https://arxiv.org/pdf/2509.18824)[speculative decoding]
* [DepthLM: Metric Depth From Vision Language Models](https://arxiv.org/pdf/2509.25413)
* [dVLA: Diffusion Vision-Language-Action Model with Multimodal Chain-of-Thought](https://arxiv.org/pdf/2509.25681)
* [Hybrid Training for Vision-Language-Action Models](https://arxiv.org/pdf/2510.00600)[VLA]
* [VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators](https://arxiv.org/pdf/2510.00406)[VLA/world model/RL]
* [ResAD: Normalized Residual Trajectory Modeling for End-to-End Autonomous Driving](https://arxiv.org/pdf/2510.08562)[E2E]
* [Agent Learning via Early Experience](https://arxiv.org/abs/2510.08558)[]

* [☆☆☆][DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving](https://arxiv.org/pdf/2510.12796)[VLA/E2E/World Model]
* [RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training](https://arxiv.org/pdf/2510.06710)[VLA/RL/Infra]
* [BLIP3o-NEXT: Next Frontier of Native Image Generation](https://arxiv.org/pdf/2510.15857)
* [MRASfM: Multi-Camera Reconstruction and Aggregation through Structure-from-Motion in Driving Scenes](https://arxiv.org/pdf/2510.15467)
* [DeepSeek-OCR: Contexts Optical Compression](https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf)
* [DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment](https://arxiv.org/pdf/2510.17148v1)
* [From Spatial to Actions: Grounding Vision-LanguageAction Model in Spatial Foundation Priors](https://arxiv.org/pdf/2510.17439)
* [Dexbotic: Open-Source Vision-Language-Action Toolbox](https://dexbotic.com/dexbotic_tech_report.pdf)[VLA框架]
* [Training-Free Group Relative Policy Optimization](https://arxiv.org/pdf/2510.08191)[RL/GRPO]
* [GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning](https://arxiv.org/pdf/2507.01006)[多模态基座]
* [MiMo-VL Technical Report](https://arxiv.org/pdf/2506.03569)[多模态基座]
* [Kwai Keye-VL Technical Report](https://arxiv.org/pdf/2507.01949)[多模态基座]
* [Kwai Keye-VL 1.5 Technical Report](https://www.alphaxiv.org/abs/2509.01563)[多模态基座]
* [Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence](https://arxiv.org/pdf/2505.23747)[3D+VLM+RL]
* [DanceGRPO: Unleashing GRPO on Visual Generation](https://arxiv.org/pdf/2505.07818)[Image Generation/RL/GRPO]
* [InfLVG: Reinforce Inference-Time Consistent Long Video Generation with GRPO](https://arxiv.org/pdf/2505.17574)[Video Generation/RL/GRPO]
* [GigaBrain-0: A World Model-Powered Vision-LanguageAction Model](https://arxiv.org/pdf/2510.19430)[VLA/world model]
* [Unified Reinforcement and Imitation Learning for Vision-Language Models](https://arxiv.org/pdf/2510.19307)
* [Diffusion Transformers with Representation Autoencoders](https://arxiv.org/pdf/2510.11690)[RAE]
* [Latent Diffusion Model without Variational Autoencoder](https://arxiv.org/pdf/2510.15301)[SVG]
* [VLM-FO1: Bridging the Gap Between High-Level Reasoning and Fine-Grained Perception in VLMs](https://www.arxiv.org/pdf/2509.25916)[VLM+目标检测]
* [VDRive: Leveraging Reinforced VLA and Diffusion Policy for End-to-end Autonomous Driving](https://arxiv.org/pdf/2510.15446)[VLA+E2E/RL/Diffusion]
* [Robot Learning: A Tutorial](https://www.arxiv.org/abs/2510.12403v1)
* [SmolVLA: A vision-language-action model for affordable and efficient robotics](https://arxiv.org/pdf/2506.01844)
* [FAST-DLLM V2: Efficient Block-Diffusion LLM](https://arxiv.org/pdf/2509.26328)
* [VLA-R1: Enhancing Reasoning in Vision-Language-Action Models](https://arxiv.org/pdf/2510.01623)
* [Scalable Vision-Language-Action Model Pretraining for Robotic Manipulation with Real-Life Human Activity Videos](https://arxiv.org/pdf/2510.21571)
* [Ctrl-World: A Controllable Generative World Model for Robot Manipulation](https://arxiv.org/pdf/2510.10125)[VLA/World Model]
* [☆][Retaining by Doing: The Role of On-Policy Data in Mitigating Forgetting](https://arxiv.org/pdf/2510.18874)  [  Link](https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247710588&idx=1&sn=90abf9a023983b44d9a1249c7c6f8fe3&chksm=977b0f68138e2c0df5e2074e0932aca387a9f09329968a1ff9d1df5e184454fbb996f340ca29&mpshare=1&scene=1&srcid=10270pwr00rYLVfFaIAWtJju&sharer_shareinfo=1a25c6434ed2b09a42b1cb1acf8c529e&sharer_shareinfo_first=1a25c6434ed2b09a42b1cb1acf8c529e#rd)
* [The Art of Scaling Reinforcement Learning Compute for LLMs](https://arxiv.org/pdf/2510.13786)
* [LongCat-Video Technical Report](https://arxiv.org/pdf/2510.22200)
* [☆☆][VOLD: Reasoning Transfer from LLMs to Vision-Language Models via On-Policy Distillation](https://arxiv.org/pdf/2510.23497)
* [☆][UrbanVLA: A Vision-Language-Action Model for Urban Micromobility](https://arxiv.org/pdf/2510.23576)
* [☆][On-Policy Distillation](https://thinkingmachines.ai/blog/on-policy-distillation/)
* [A Survey on Efficient Vision-Language-Action Models](https://arxiv.org/pdf/2510.24795)[VLA/加速]
* [DeepEyesV2: Toward Agentic Multimodal Model](https://arxiv.org/pdf/2511.05271)
* [Visual Spatial Tuning](https://arxiv.org/pdf/2511.05491)
* [Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment](https://arxiv.org/pdf/2511.04555)
* [WOD-E2E: Waymo Open Dataset for End-to-End Driving in Challenging Long-tail Scenarios](https://arxiv.org/pdf/2510.26125)
* [☆☆☆☆☆][Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail](https://arxiv.org/pdf/2511.00088)[VLA/E2E/COT]
* [SpatialThinker: Reinforcing 3D Reasoning in Multimodal LLMs via Spatial Rewards](https://arxiv.org/pdf/2511.07403)
* [Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised Reinforcement Learning](https://arxiv.org/pdf/2510.27606)
* [GRPO-Guard: Mitigating Implicit Over-Optimization in Flow Matching via Regulated Clipping](https://arxiv.org/abs/2510.22319)
* [☆☆][Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process](https://arxiv.org/pdf/2511.01718)[VLA/world model]
* [☆][Running VLAs at Real-time Speed](https://arxiv.org/pdf/2510.26742)[VLA/加速/部署]
* [☆][Kimi Linear: An Expressive, Efficient Attention Architecture](https://arxiv.org/pdf/2510.26692)[线性注意力]
* [☆][Qwen3-next](https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&from=research.latest-advancements-list)[线性注意力]
* [☆☆][OmniNWM: Omniscient Driving Navigation World Models](https://www.arxiv.org/pdf/2510.18313)[VLA/E2E/world model]
* [☆][Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model](https://arxiv.org/pdf/2510.27607)[VLA/world model]
* [☆][LightReasoner: Can Small Language Models Teach Large Language Models Reasoning?](https://arxiv.org/pdf/2510.07962)
* [Flow Matching-Based Autonomous Driving Planning with Advanced Interactive Behavior Modeling](https://arxiv.org/pdf/2510.11083)[E2E]
* [☆][Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding](arxiv.org/pdf/2510.06308)[理解&生成/Discrete Diffusion]
* [☆☆][Efficient Multi-Camera Tokenization with Triplanes for End-to-End Driving](https://arxiv.org/pdf/2506.12251)[E2E/长期记忆/多视角时序编码/VLA加速]
* [☆][Towards efficient and effective multi-camera encoding for end-to-end driving](https://jiawei-yang.github.io/Flex/#:~:text=We%20introduce%20Flex%2C%20a%20scene%20encoder%20for%20end-to-end,addresses%20the%20cost%20of%20processing%20high-bandwidth%20multi-camera%20inputs.)[E2E/长期记忆/多视角时序编码/VLA加速]
* [☆][DepthVLA: Enhancing Vision-Language-Action Models with Depth-Aware Spatial Reasoning](https://arxiv.org/pdf/2510.13375)[VLA/空间感知]











