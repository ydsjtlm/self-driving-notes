# LLM
* [☆X][ChatGPT is not all you need. A State of the Art Review of large Generative AI models](https://arxiv.org/abs/2301.04655)
* [☆X][Multimodal Deep Learning](https://arxiv.org/pdf/2301.04856.pdf)
* [☆][Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)  [【阅读笔记】](https://zhuanlan.zhihu.com/p/641016232)
* [☆][Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)  [【阅读笔记】](https://zhuanlan.zhihu.com/p/634627008)
* [☆][BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/638895437)
* [☆][GPT1: Improving Language Understanding by Generative Pre-Training](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)  [【阅读笔记】](https://zhuanlan.zhihu.com/p/638298091)
* [☆][GPT2: Language Models are Unsupervised Multitask Learners](https://life-extension.github.io/2020/05/27/GPT%E6%8A%80%E6%9C%AF%E5%88%9D%E6%8E%A2/language-models.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/638298091)
* [☆][GPT3: Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)  [【阅读笔记】](https://zhuanlan.zhihu.com/p/638298091)
* [☆][InstructGPT: Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155.pdf)  [【阅读笔记】](https://zhuanlan.zhihu.com/p/638298091)
* [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension]()
* [Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556.pdf)
* [Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361.pdf)

* [☆][ViT: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/pdf/2010.11929.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/637849428)
* [☆][Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/pdf/2103.14030.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/637849428)
* [☆][DETR: End-to-End Object Detection with Transformers](https://arxiv.org/pdf/2005.12872.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/637849428)
* [☆][Deformable DETR Deformable Transformers for End-to-End Object Detection](https://arxiv.org/pdf/2010.04159.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/637849428)
* [Visual Programming: Compositional visual reasoning without training](https://arxiv.org/pdf/2211.11559.pdf)
* [Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models](https://arxiv.org/pdf/2306.08641.pdf)

# AIGC
* [☆] [VQGAN: Taming Transformers for High-Resolution Image Synthesis](https://arxiv.org/pdf/2012.09841.pdf)[【阅读笔记】](https://zhuanlan.zhihu.com/p/641471772)
* [☆] [DDPM: Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239.pdf)[【阅读笔记】](https://zhuanlan.zhihu.com/p/641471772)
* [☆] [DALL-E2: Hierarchical Text-Conditional Image Generation with CLIP Latents](https://arxiv.org/pdf/2204.06125.pdf)[【阅读笔记】](https://zhuanlan.zhihu.com/p/641471772)
* [☆] [DALL-E: Zero-Shot Text-to-Image Generation](https://arxiv.org/pdf/2102.12092.pdf)[【阅读笔记】](https://zhuanlan.zhihu.com/p/641471772)
* [☆] [Stable Diffusion: High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/pdf/2112.10752.pdf)[【阅读笔记】](https://zhuanlan.zhihu.com/p/641471772)
* [Imagen: Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding](https://arxiv.org/pdf/2205.11487.pdf)[【阅读笔记】](https://zhuanlan.zhihu.com/p/641471772)
* [☆] [RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths](https://arxiv.org/pdf/2305.18295.pdf)[【阅读笔记】](https://zhuanlan.zhihu.com/p/641471772)
* [☆] [CM3leon: Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning](https://scontent-lax3-1.xx.fbcdn.net/v/t39.2365-6/358725877_789390529544546_1176484804732743296_n.pdf?_nc_cat=108&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=_diQr9c6Ru8AX8AL77d&_nc_ht=scontent-lax3-1.xx&oh=00_AfD8JGolEO3QmSnrkpzQnbv533aSj2eHNm0at6cJ2epOpg&oe=64BB4972)
* [VideoControlNet: A Motion-Guided Video-to-Video Translation Framework by Using Diffusion Model with ControlNet](https://arxiv.org/pdf/2307.14073.pdf)
* [Control-A-Video: Controllable Text-to-Video Generation with Diffusion Models](https://arxiv.org/pdf/2305.13840.pdf)
* [VideoGPT: Video Generation using VQ-VAE and Transformers](https://arxiv.org/pdf/2104.10157.pdf)
* [Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets](https://arxiv.org/pdf/2311.15127)
* [Generative Image as Action Models](https://arxiv.org/pdf/2407.07875)
* [Lumina-T2X: Transforming Text into Any Modality, Resolution, and Duration via Flow-based Large Diffusion Transformers](https://arxiv.org/pdf/2405.05945)
* [DriveDiTFit: Fine-tuning Diffusion Transformers for Autonomous Driving](https://arxiv.org/pdf/2407.15661v1)
* [HoloDreamer: Holistic 3D Panoramic World Generation from Text Descriptions](https://arxiv.org/pdf/2407.15187)
* [CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer](https://arxiv.org/pdf/2408.06072)
* [Panacea: Panoramic and Controllable Video Generation for Autonomous Driving](https://arxiv.org/pdf/2311.16813)
* [Panacea+: Panoramic and Controllable Video Generation for Autonomous Driving](https://arxiv.org/pdf/2408.07605)
* [ControlNeXt: Powerful and Efficient Control for Image and Video Generation](https://arxiv.org/pdf/2408.06070)
* [xGen-VideoSyn-1: High-fidelity Text-to-Video Synthesis with Compressed Representations](https://arxiv.org/pdf/2408.12590v1)
* [Latte: Latent Diffusion Transformer for Video Generation](https://arxiv.org/pdf/2401.03048)
* [OmniGen: Unified Image Generation](https://arxiv.org/pdf/2409.11340v1)
* [DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation](https://arxiv.org/pdf/2410.13571v1)
* [FreeVS: Generative View Synthesis on Free Driving Trajectory](https://arxiv.org/pdf/2410.18079v1)
* [X-DRIVE: CROSS-MODALITY CONSISTENT MULTISENSOR DATA SYNTHESIS FOR DRIVING SCENARIOS](https://arxiv.org/pdf/2411.01123v1)
* [Randomized Autoregressive Visual Generation](https://arxiv.org/pdf/2411.00776)
* [HunyuanVideo: A Systematic Framework For Large Video Generative Models](https://github.com/Tencent/HunyuanVideo)
* [Infinity∞: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis](https://arxiv.org/pdf/2412.04431)
* [You See it, You Got it: Learning 3D Creation on Pose-Free Videos at Scale](https://arxiv.org/pdf/2412.06699)


# Multimodal (Vision/Language/Audio)
* [☆] [Segment Anything](https://arxiv.org/pdf/2304.02643.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/630808078)
* [☆] [Segment and Track Anything](https://arxiv.org/pdf/2305.06558.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/630987885)
* [☆] [Track Anything: Segment Anything Meets Videos](https://arxiv.org/pdf/2304.11968.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/630987885)
* [☆] [CLIP: Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020.pdf)[【阅读笔记】](https://zhuanlan.zhihu.com/p/631046116)
* [☆] [ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://arxiv.org/pdf/2102.03334.pdf)[【阅读笔记】](https://zhuanlan.zhihu.com/p/635802911)
* [☆] [ALBEF: Align before Fuse: Vision and Language Representation Learning with Momentum Distillation](https://arxiv.org/pdf/2107.07651.pdf)  [【阅读笔记】](https://zhuanlan.zhihu.com/p/635127422/edit)
* [☆] [VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts](https://arxiv.org/pdf/2111.02358.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/635116412)
* [☆] [Meta-Transformer: A Unified Framework for Multimodal Learning](https://arxiv.org/pdf/2307.10802.pdf) [[阅读笔记]](https://zhuanlan.zhihu.com/p/645727989)
* [☆] [ImageBind: One Embedding Space To Bind Them All](https://arxiv.org/pdf/2305.05665.pdf) [[阅读笔记]](https://zhuanlan.zhihu.com/p/645780641)
* [☆] [beitv3: Image as a Foreign Language BEiT Pretraining for All Vision and Vision-Language Tasks](https://arxiv.org/pdf/2208.10442.pdf) [[阅读笔记]](https://zhuanlan.zhihu.com/p/645983189)
* [☆] [PandaGPT: One Model To Instruction-Follow Them All](https://arxiv.org/pdf/2305.16355.pdf) [[阅读笔记]](https://zhuanlan.zhihu.com/p/645780641)
* [☆] [KOSMOS: Language is not all you Need Aligning Perception with Language Models](https://arxiv.org/pdf/2302.14045.pdf)[【阅读笔记】](https://zhuanlan.zhihu.com/p/631396292)
* [☆] [KOSMOS-2: Grounding Multimodal Large Language Models to the World](https://arxiv.org/pdf/2306.14824.pdf)[【阅读笔记】](https://zhuanlan.zhihu.com/p/631396292)
* [☆] [LLaVA: Visual Instruction Tuning](https://arxiv.org/pdf/2304.08485.pdf)[【阅读笔记】](https://zhuanlan.zhihu.com/p/636584622)
* [☆] [Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models](https://arxiv.org/pdf/2306.05424.pdf)[【阅读笔记】](https://zhuanlan.zhihu.com/p/636584622)
* [☆] [PG-Video-LLaVA: Pixel Grounding Large Video-Language Models](https://arxiv.org/pdf/2311.13435.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/636584622)
* [☆] [Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding](https://arxiv.org/pdf/2306.02858.pdf)[【阅读笔记】](https://zhuanlan.zhihu.com/p/636854782)  
* [☆] [MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models](https://arxiv.org/pdf/2304.10592.pdf)[【阅读笔记】](https://zhuanlan.zhihu.com/p/636854782)  
* [☆] [BLIP2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/pdf/2301.12597.pdf)[【阅读笔记】](https://zhuanlan.zhihu.com/p/636854782)    [[Code]](https://github.com/salesforce/LAVIS)
* [☆] [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://arxiv.org/pdf/2305.06500.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/636854782)   [[Code]](https://github.com/salesforce/LAVIS)
* [☆] [Flamingo: a Visual Language Model for Few-Shot Learning](https://arxiv.org/pdf/2204.14198.pdf) [[阅读笔记]](https://zhuanlan.zhihu.com/p/645750137)
* [☆] [PaLM-E An Embodied Multimodal Language Model](https://arxiv.org/pdf/2303.03378.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/637430937)
* [☆] [RT-2 Vision-Language-Action Models Transfer Web Knowledge to Robotic Control](https://robotics-transformer2.github.io/assets/rt2.pdf)[[阅读笔记]](https://zhuanlan.zhihu.com/p/646729912)
* [☆] [VisionLLM Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks](https://arxiv.org/pdf/2305.11175.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/639467790)
* [☆] [InternGPT: Solving Vision-Centric Tasks by Interacting with ChatGPT Beyond Language](https://arxiv.org/pdf/2305.05662.pdf)  [【阅读笔记】](https://zhuanlan.zhihu.com/p/639505432)
* [☆] [Segment Any Point Cloud Sequences by Distilling Vision Foundation Models](https://arxiv.org/pdf/2306.09347.pdf)  [【阅读笔记】](https://zhuanlan.zhihu.com/p/640760232)
* [☆] [综述 A Survey on Multimodal Large Language Models](https://arxiv.org/pdf/2306.13549.pdf)  [[Github]](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)  [【阅读笔记】](https://zhuanlan.zhihu.com/p/641261865)
* [☆] [综述 A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223.pdf) [[阅读笔记]](https://zhuanlan.zhihu.com/p/642003921)
* [☆] [评测：MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models](https://arxiv.org/pdf/2306.13394.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/641261865)
* [☆] [评测：C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models](https://arxiv.org/pdf/2305.08322.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/641261865)
* [☆X][Vision Language Transformers: A Survey](https://arxiv.org/pdf/2307.03254.pdf)
* [☆] [VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models](https://voxposer.github.io/voxposer.pdf)   [【阅读笔记】](https://zhuanlan.zhihu.com/p/644424405)
* [☆] [Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks](https://arxiv.org/pdf/2206.08916.pdf)   [[阅读笔记]](https://zhuanlan.zhihu.com/p/644096922)
* [☆] [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/pdf/2307.09288.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/645230336)
* [☆] [LLaMA Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971.pdf)[【阅读笔记】](https://zhuanlan.zhihu.com/p/642719013)
* [☆] [Scaling Instruction-Finetuned Language Models](https://arxiv.org/pdf/2210.11416.pdf)  [[阅读笔记]](https://zhuanlan.zhihu.com/p/645317908) 
* [☆] [LONGNET: Scaling Transformers to 1,000,000,000 Tokens](https://arxiv.org/pdf/2307.02486.pdf) [[阅读笔记]](https://zhuanlan.zhihu.com/p/645326943)
* [☆] [Faster Segment Anything: Towards Lightweight SAM for Mobile Applications](https://arxiv.org/pdf/2306.14289.pdf) [[阅读笔记]](https://zhuanlan.zhihu.com/p/645714778)  
* [☆] [Shikra: Unleashing Multimodal LLM’s Referential Dialogue Magic](https://arxiv.org/pdf/2306.15195.pdf) [[阅读笔记]](https://zhuanlan.zhihu.com/p/645787351)
* [☆] [Textbooks Are All You Need](https://arxiv.org/pdf/2306.11644.pdf)
* [☆] [3D-LLM: Injecting the 3D World into Large Language Models](https://arxiv.org/pdf/2307.12981.pdf) [[阅读笔记]](https://zhuanlan.zhihu.com/p/646624803)
* [☆] [LoRA Low-Rank Adaptation of Large Language Models](https://arxiv.org/pdf/2106.09685.pdf) [[阅读笔记]](https://zhuanlan.zhihu.com/p/646629579)
* [☆] [AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn](https://arxiv.org/pdf/2306.08640.pdf) [[阅读笔记]](https://zhuanlan.zhihu.com/p/646689611)
* [☆] [HuggingGPT Solving AI Tasks with ChatGPT and its Friends in HuggingFace](https://arxiv.org/pdf/2303.17580.pdf) [[阅读笔记]](https://zhuanlan.zhihu.com/p/646689611)
* [☆] [NExT-GPT: Any-to-Any Multimodal LLM](https://arxiv.org/pdf/2309.05519.pdf) [[阅读笔记]](https://zhuanlan.zhihu.com/p/661419123)
* [☆] [MINIGPT-5: INTERLEAVED VISION-AND-LANGUAGE GENERATION VIA GENERATIVE VOKENS](https://browse.arxiv.org/pdf/2310.02239v1.pdf) [[阅读笔记]](https://zhuanlan.zhihu.com/p/661419123)


* [CLIP分割：Language-driven Semantic Segmentation](https://arxiv.org/pdf/2201.03546.pdf)
* [CLIP分割：GroupViT: Semantic Segmentation Emerges from Text Supervision](https://arxiv.org/pdf/2202.11094.pdf)
* [CLIP检测：Open-vocabulary Object Detection via Vision and Language Knowledge Distillation](https://arxiv.org/pdf/2104.13921.pdf)
* [CLIP检测：Grounded Language-Image Pre-training](https://arxiv.org/pdf/2112.03857.pdf)
* [CLIPasso Semantically-Aware Object Sketching](https://arxiv.org/pdf/2202.05822.pdf)
* [Contextual Object Detection with Multimodal Large Language Models](https://arxiv.org/pdf/2305.18279.pdf) [[Code]](https://github.com/yuhangzang/ContextDET)

* [QLoRA Efficient Finetuning of Quantized LLMs](https://arxiv.org/pdf/2305.14314.pdf)


* [LIMA Less Is More for Alignment](https://arxiv.org/pdf/2305.11206.pdf)
* [FrugalGPT How to Use Large Language Models While Reducing Cost and Improving Performance](https://arxiv.org/pdf/2305.05176.pdf)
* [SELF-INSTRUCT Aligning Language Models with Self-Generated Instructions](https://arxiv.org/pdf/2212.10560.pdf)
* [Toolformer Language Models Can Teach Themselves to Use Tools](https://arxiv.org/pdf/2302.04761.pdf)
* [Tree of Thoughts Deliberate Problem Solving with Large Language Models](https://arxiv.org/pdf/2305.10601.pdf)
* [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/pdf/2201.11903.pdf)

* [Whisper: Robust Speech Recognition via Large-Scale Weak Supervision](https://arxiv.org/pdf/2212.04356.pdf)
* [LaMDA Language Models for Dialog Applications](https://arxiv.org/pdf/2201.08239.pdf)

* [BLIP Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/pdf/2201.12086.pdf)

* [CoCa Contrastive Captioners are Image-Text Foundation Models](https://arxiv.org/pdf/2205.01917.pdf)
* [Fine-Tuning Language Models from Human Preferences](https://arxiv.org/pdf/1909.08593.pdf)
* [Anthropic LLM: Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/pdf/2204.05862.pdf)
* [GPTs are GPTs An Early Look at the Labor Market Impact Potential of Large Language Models](https://arxiv.org/pdf/2303.10130.pdf)
* [sparks of artificial general intelligence](https://arxiv.org/pdf/2303.12712.pdf)


* [Transformer models: an introduction and catalog](https://arxiv.org/pdf/2302.07730.pdf)
* [A Survey on Long Text Modeling with Transformers](https://arxiv.org/pdf/2302.14502.pdf)
* [A Comprehensive Survey on Pretrained Foundation Models A History from BERT to ChatGPT](https://arxiv.org/pdf/2302.09419.pdf)
* [Large Language Models Can Self-Improve](https://arxiv.org/pdf/2210.11610.pdf)

* [MovieChat From Dense Token to Sparse Memory for Long Video Understanding](https://arxiv.org/pdf/2307.16449.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/636854782)
* [LLM-GROUNDED VIDEO DIFFUSION MODELS](https://arxiv.org/pdf/2309.17444.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/661443505)
* [Code Llama: Open Foundation Models for Code](https://arxiv.org/pdf/2308.12950.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/661685730)
* [Siamese Masked Autoencoders](https://siam-mae-video.github.io/resources/paper.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/661683294)
* [MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning](https://arxiv.org/pdf/2310.09478.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/661892549)
* [Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation](https://arxiv.org/pdf/2308.07931.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/667631161)
* [Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models](https://arxiv.org/pdf/2303.04803.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/667633652)
* [SegPrompt: Boosting Open-world Segmentation via Category-level Prompt Learning](https://arxiv.org/pdf/2308.06531.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/667636980)
* [UniDoc: A Universal Large Multimodal Model for Simultaneous Text Detection, Recognition, Spotting and Understanding](https://arxiv.org/pdf/2308.11592.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/667639160)
* [NavGPT: Beyond Segmentation: Road Network Generation with Multi-Modal LLMs](https://arxiv.org/pdf/2310.09755.pdf) [【阅读笔记】]()
* [LISA: Reasoning Segmentation via Large Language Model](https://arxiv.org/pdf/2308.00692.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/667646754)
* [Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V](https://arxiv.org/pdf/2310.11441.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/667651095)
* [UnifiedVisionGPT: Streamlining Vision-Oriented AI through Generalized Multimodal Framework](https://arxiv.org/pdf/2311.10125.pdf) [【阅读笔记】](NoMore) 
* [Towards Open-Ended Visual Recognition with Large Language Model](https://arxiv.org/pdf/2311.08400.pdf) [【阅读笔记】]()
* [ShareGPT4V: Improving Large Multi-Modal Models with Better Captions](https://arxiv.org/pdf/2311.12793.pdf) [【阅读笔记】]()
* [An Embodied Generalist Agent in 3D World](https://arxiv.org/pdf/2311.12871.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/668856211)
* [The Curse of Recursion: Training on Generated Data Makes Models Forget](https://arxiv.org/pdf/2305.17493.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/668869579)
* [Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks](https://arxiv.org/pdf/2311.06242.pdf) [【阅读笔记】](https://zhuanlan.zhihu.com/p/668875847)


* [An Early Evaluation of GPT-4V(ision)](https://arxiv.org/pdf/2310.16534.pdf)
* [Video Language Planning](https://arxiv.org/pdf/2310.10625.pdf)
* [Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding](https://arxiv.org/pdf/2311.08046.pdf)
* [Frozen Transformers in Language Models Are Effective Visual Encoder Layers](https://arxiv.org/pdf/2310.12973.pdf)
* [On Efficient Training of Large-Scale Deep Learning Models: A Literature Review](https://arxiv.org/pdf/2304.03589.pdf)
* [GPT4Video: A Unified Multimodal Large Language Model for lnstruction-Followed Understanding and Safety-Aware Generation](https://arxiv.org/pdf/2311.16511.pdf)
* [Sequential Modeling Enables Scalable Learning for Large Vision Models](https://arxiv.org/pdf/2312.00785.pdf)
* [LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models](https://arxiv.org/pdf/2312.02949.pdf)
* [Gemini: A Family of Highly Capable Multimodal Models](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf)
* [Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning](https://arxiv.org/pdf/2311.17842.pdf)
* [Chain of Code: Reasoning with a Language Model-Augmented Code Emulator](https://arxiv.org/pdf/2312.04474.pdf)
* [WEAK-TO-STRONG GENERALIZATION: ELICITING STRONG CAPABILITIES WITH WEAK SUPERVISION](https://cdn.openai.com/papers/weak-to-strong-generalization.pdf)
* [LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models](https://arxiv.org/pdf/2311.17043.pdf)
* [VideoPoet: A Large Language Model for Zero-Shot Video Generation](https://storage.googleapis.com/videopoet/paper.pdf)
* [PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU](https://ipads.se.sjtu.edu.cn/_media/publications/powerinfer-20231219.pdf)
* [A Simple Recipe for Contrastively Pre-training Video-First Encoders Beyond 16 Frames](https://arxiv.org/pdf/2312.07395.pdf)
* [Mixtral of Experts](https://arxiv.org/pdf/2401.04088.pdf)
* [MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World](https://arxiv.org/pdf/2401.08577.pdf)
* [CognitiveDog: Large Multimodal Model Based System to Translate Vision and Language into Action of Quadruped Robot](https://arxiv.org/pdf/2401.09388.pdf)
* [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/ftp/arxiv/papers/2312/2312.00752.pdf)
* [LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers](https://arxiv.org/pdf/2310.03294.pdf)
* [LLM in a flash: Efficient Large Language Model Inference with Limited Memory](https://arxiv.org/pdf/2312.11514.pdf)
* [AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling](https://arxiv.org/pdf/2402.12226.pdf)
* [Model Composition for Multimodal Large Language Models](https://arxiv.org/pdf/2402.12750.pdf)
* [Lumiere: A Space-Time Diffusion Model for Video Generation](https://arxiv.org/pdf/2401.12945.pdf)
* [EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought](https://arxiv.org/pdf/2305.15021.pdf)
* [Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models](https://arxiv.org/pdf/2402.17177.pdf)
* [DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models](https://arxiv.org/pdf/2403.00818.pdf)
* [RT-H: Action Hierarchies Using Language](https://arxiv.org/pdf/2403.01823.pdf)
* [VideoMamba: State Space Model for Efficient Video Understanding](https://arxiv.org/pdf/2403.06977.pdf)
* [GiT: Towards Generalist Vision Transformer through Universal Language Interface](https://arxiv.org/pdf/2403.09394.pdf)
* [Mora: Enabling Generalist Video Generation via A Multi-Agent Framework](https://arxiv.org/pdf/2403.13248.pdf)
* [CLIP-BEVFormer: Enhancing Multi-View Image-Based BEV Detector with Ground Truth Flow](https://arxiv.org/pdf/2403.08919.pdf)
* [MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens](https://arxiv.org/pdf/2404.03413.pdf)
* [VAR_Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction](https://arxiv.org/pdf/2404.02905.pdf)
* [Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models](https://arxiv.org/pdf/2403.18814.pdf)
* [Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation](https://arxiv.org/pdf/2406.06525)
* [OpenVLA: An Open-Source Vision-Language-Action Model](https://arxiv.org/pdf/2406.09246)
* [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/pdf/2205.14135)
* [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://tridao.me/publications/flash2/flash2.pdf)
* [FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision](https://tridao.me/publications/flash3/flash3.pdf)
* [Unveiling Encoder-Free Vision-Language Models](https://arxiv.org/pdf/2406.11832)
* [DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of MLLM](https://arxiv.org/pdf/2403.12488)
* [LUMINA-MGPT: ILLUMINATE FLEXIBLE PHOTOREALISTIC TEXT-TO-IMAGE GENERATION WITH MULTIMODAL GENERATIVE PRETRAINING](https://arxiv.org/pdf/2408.02657v1)
* [DRIVEARENA: A Closed-loop Generative Simulation Platform for Autonomous Driving](https://arxiv.org/pdf/2408.00415v1)
* [The Llama 3 Herd of Models](https://arxiv.org/pdf/2407.21783v1)
* [VLM][Autoregressive Image Generation without Vector Quantization](https://arxiv.org/pdf/2406.11838)
* [VLM][Chameleon: Mixed-Modal Early-Fusion Foundation Models](https://arxiv.org/pdf/2405.09818)
* [VLM][Language Model Beats Diffusion Tokenizer is key to visual generation](https://arxiv.org/pdf/2310.05737v2)
* [DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search](https://www.arxiv.org/pdf/2408.08152)
* [VLM][Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model](https://arxiv.org/pdf/2408.11039v1)
* [VLM][MaVEn: An Effective Multi-granularity Hybrid Visual Encoding Framework for Multimodal Large Language Model](https://arxiv.org/pdf/2408.12321v1)
* [VLM][xGen-MM (BLIP-3): A Family of Open Large Multimodal Models](https://www.arxiv.org/pdf/2408.08872)
* [VLM][Building and better understanding vision-language models: insights and future directions](https://arxiv.org/pdf/2408.12637v1)
* [Large Language Monkeys: Scaling Inference Compute with Repeated Sampling](https://arxiv.org/pdf/2407.21787)
* [Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters](https://arxiv.org/pdf/2408.03314)
* [VLM][NVLM: Open Frontier-Class Multimodal LLMs](https://arxiv.org/pdf/2409.11402v1)
* [Qwen2-VL: Enhancing Vision-Language Model’s Perception of the World at Any Resolution](https://arxiv.org/pdf/2409.12191v1)
* [Emu3: Next-Token Prediction is All You Need](https://arxiv.org/pdf/2409.18869)
* [Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation](https://arxiv.org/pdf/2410.13848)
* [xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video Even in VLMs](https://arxiv.org/pdf/2410.16267v1)
* [☆][Q-VLM: Post-training Quantization for Large Vision-Language Models](https://arxiv.org/pdf/2410.08119v1)[VLM部署/VLM提速]
* [☆][SparseLLM: Towards Global Pruning of Pre-trained Language Models](https://arxiv.org/abs/2402.17946)[LLM部署/剪枝]
* [☆][Loong: Generating Minute-level Long Videos with Autoregressive Language Models](https://arxiv.org/pdf/2410.02757v1)
* [☆][LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding](https://arxiv.org/pdf/2410.17434v1)
* [☆][OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models](https://arxiv.org/pdf/2411.04905)
* [☆][VLA][CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation](https://arxiv.org/pdf/2411.19650v1)
* [☆][VLA][CombatVLA: An Efficient Vision-Language-Action Model for Combat Tasks in 3D Action Role-Playing Games](https://arxiv.org/pdf/2503.09527)
* [☆][VLA][HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model](https://arxiv.org/pdf/2503.10631)
* [☆][VLA][ChatVLA: Unified Multimodal Understanding and Robot Control with Vision-Language-Action Model](https://arxiv.org/pdf/2502.14420)
* [☆][VLA][Diffusion-VLA: Scaling Robot Foundation Models via Unified Diffusion and Autoregression](https://arxiv.org/pdf/2412.03293)[code](https://github.com/juruobenruo/DexVLA)
* [☆][VLA][DexVLA: Vision-Language Model with Plug-In Diffusion Expert for General Robot Control](https://arxiv.org/pdf/2502.05855)
* [☆☆][VLA][Scaling Diffusion Policy in Transformer to 1 Billion Parameters for Robotic Manipulation](https://arxiv.org/pdf/2409.14411)
* [☆][VLA][OpenDriveVLA: Towards End-to-end Autonomous Driving with Large Vision Language Action Model](https://arxiv.org/pdf/2503.23463)
* [☆][Multimodal Latent Language Modeling with Next-Token Diffusion](https://arxiv.org/pdf/2412.08635)
* [☆][DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding](https://arxiv.org/pdf/2412.10302v1)
* [☆][Byte Latent Transformer: Patches Scale Better Than Tokens](https://arxiv.org/pdf/2412.09871)
* [☆][FastVLM: Efficient Vision Encoding for Vision Language Models](https://arxiv.org/pdf/2412.13303)[VLM部署/VLM提速]
* [☆][LLM Post-Training: A Deep Dive into Reasoning Large Language Models](https://arxiv.org/pdf/2502.21321)[post-training/综述]
* [☆][Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/ORZ_paper.pdf)
* [☆][Visual-RFT: Visual Reinforcement Fine-Tuning](https://arxiv.org/pdf/2503.01785)
* [☆][Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models](https://arxiv.org/pdf/2503.06749)
* [☆][MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale Reinforcement Learning](https://arxiv.org/pdf/2503.07365)
* [☆][Predictable Scale: Part I — Optimal Hyperparameter Scaling Law in Large Language Model Pretraining](https://arxiv.org/pdf/2503.04715)
* [☆][Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models](https://arxiv.org/pdf/2503.09573)
* [☆][Understanding R1-Zero-Like Training: A Critical Perspective](https://github.com/sail-sg/understand-r1-zero/blob/main/understand-r1-zero.pdf)
* [☆][Video-T1: Test-Time Scaling for Video Generation](https://arxiv.org/pdf/2503.18942)
* [☆][EfficientLLaVA:Generalizable Auto-Pruning for Large Vision-language Models](https://arxiv.org/pdf/2503.15369) [VLM部署/VLM提速/剪枝]
* [☆][SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language Models for Long-Form Video Understanding](https://arxiv.org/pdf/2503.18943) [长视频理解/]
* [Unified Autoregressive Visual Generation and Understanding with Continuous Tokens](https://arxiv.org/pdf/2503.13436)
* [DAPO: An Open-Source LLM Reinforcement Learning System at Scale](https://dapo-sia.github.io/static/pdf/dapo_paper.pdf)
* [Qwen2.5-Omni Technical Report](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/assets/Qwen2.5_Omni.pdf)
* [Video-R1: Reinforcing Video Reasoning in MLLMs](https://arxiv.org/pdf/2503.21776)
* [☆][MAGI-1: Autoregressive Video Generation at Scale](https://static.magi.world/static/files/MAGI_1.pdf)[TBD/视频生成]
* [☆][Bridging Continuous and Discrete Tokens for Autoregressive Visual Generation](https://arxiv.org/pdf/2503.16430)[后训练量化自回归生成token离散化问题]
* [☆][Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs](https://arxiv.org/pdf/2503.01307)[基座模型边界锁死RL能力上限]
* [☆][Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?](https://arxiv.org/pdf/2504.13837)[基座模型边界锁死RL能力上限]
* [☆][Rethinking Reflection in Pre-Training](https://arxiv.org/pdf/2504.04022)[基座模型边界锁死RL能力上限]
* [☆][TTRL: Test-Time Reinforcement Learning](https://arxiv.org/pdf/2504.16084)[基座模型边界锁死RL能力上限]
* [☆][Inference-Time Scaling for Generalist Reward Modeling](https://arxiv.org/pdf/2504.02495)[基座模型边界锁死RL能力上限]
* [☆][SimpleAR: Pushing the Frontier of Autoregressive Visual Generation through Pretraining, SFT, and RL](http://arxiv.org/pdf/2504.11455)[AR图像生成/偏工程性/架构简洁/训练优化/推理优化]
* [☆][Describe Anything: Detailed Localized Image and Video Captioning](https://arxiv.org/pdf/2504.16072)[图片/视频局部信息描述]
* [☆][SmolVLM: Redefining small and efficient multimodal models](https://arxiv.org/pdf/2504.05299)[模型小型化/VLM提速]
* [☆][Fast-Slow Thinking for Large Vision-Language Model Reasoning](https://arxiv.org/pdf/2504.18458)[VLM过度思考]
* [NORA: A Small Open-Sourced Generalist Vision Language Action Model for Embodied Tasks](https://arxiv.org/pdf/2504.19854)
* [TinyLLaVA: A Framework of Small-scale Large Multimodal Models](https://arxiv.org/pdf/2402.14289)
* [StreamBridge: Turning Your Offline Video Large Language Model into a Proactive Streaming Assistant](https://arxiv.org/pdf/2505.05467)
* [LTX-Video: Realtime Video Latent Diffusion](https://arxiv.org/pdf/2501.00103)
* [☆][VLM][Seed1.5-VL Technical Report](https://arxiv.org/pdf/2505.07062)
* [Flow-GRPO: Training Flow Matching Models via Online RL](https://www.arxiv.org/pdf/2505.05470)
* [Speculative Decoding Reimagined for Multimodal Large Language Models](https://arxiv.org/pdf/2505.14260)[VLM部署/VLM提速]
* [UniGen: Enhanced Training & Test-Time Strategies for Unified Multimodal Understanding and Generation](https://arxiv.org/pdf/2505.14682)
* [Vid2World: Crafting Video Diffusion Models toInteractive World Models](https://arxiv.org/pdf/2505.14357)
* [MMaDA: Multimodal Large Diffusion Language Models](https://arxiv.org/pdf/2505.15809)
* [G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language Model via Reinforcement Learning](https://arxiv.org/pdf/2505.13426)
* [VIDEORFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning](https://arxiv.org/pdf/2505.12434)
* [LLaVA-4D: Embedding SpatioTemporal Prompt into LMMs for 4D Scene Understanding](https://arxiv.org/pdf/2505.12253)
* [Interactive Post-Training for Vision-Language-Action Models](https://arxiv.org/pdf/2505.17016)
* [Multi-SpatialMLLM: Multi-Frame Spatial Understanding with MultiModal Large Language Models](https://arxiv.org/pdf/2505.17015)
* [LaViDa: A Large Diffusion Language Model for Multimodal Understanding](https://arxiv.org/pdf/2505.16839)
* [R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO](https://arxiv.org/pdf/2505.16673)
* [Dimple: Discrete Diffusion Multimodal Large Language Model with Parallel Decoding](https://arxiv.org/pdf/2505.16990)
* [Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models](https://arxiv.org/pdf/2505.10446)

